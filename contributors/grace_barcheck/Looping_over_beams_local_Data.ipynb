{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Query icepyx; see what tracks are available in area of interest <-- doesn't work, using local data instead\n",
    "\n",
    "2. Save track numbers, beams, and repeat numbers into a dictionary\n",
    "\n",
    "3. For each track/beam combination, loop over all possible repeat pairs\n",
    "\n",
    "    A. Load all beams and all repeats for that track using icepyx (?). For all beams / repeats:\n",
    "    \n",
    "        - Do whatever we are doing with ATL03\n",
    "    \n",
    "        - Fill in nan gaps with noise\n",
    "        \n",
    "    B. For each repeat pair:\n",
    "        \n",
    "        - Loop across the along track coordinates: \n",
    "        \n",
    "            Choices: window size, search width, running average window size, step, where to save data geographically\n",
    "            \n",
    "            Output: Best lag, corresponding correlation coefficient, equivalent along-track velocity\n",
    "            \n",
    "        - Save results in a hdf5 file with date collected, dx from ATL03 processing, lat, lon, veloc, correlation coefficient, best lag, # contributing nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icepyx import icesat2data as ipd\n",
    "import os, glob, re, h5py, sys, pyproj\n",
    "import matplotlib as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from astropy.time import Time\n",
    "from scipy.signal import correlate, detrend\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "import pointCollection as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/home/jovyan/shared/surface_velocity/FIS_ATL06'\n",
    "ATL06_files=glob.glob(os.path.join(datapath, '*.h5'))\n",
    "\n",
    "out_path = 'shared/surface_velocity/ATL06_out/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0080', '1131', '0232', '1031', '0634', '0507', '0131', '0192', '0354', '1061', '0492', '0690', '0970', '0187', '0558', '1335', '0741', '0659', '0894', '1183', '0680', '1101', '1168', '0034', '0568', '0705', '0293', '0711', '1040', '0070', '0543', '1244', '1192', '0314', '0126', '1193', '1147', '0253', '0451', '1122', '0994', '0391', '0141', '0979', '0476', '1223', '1137', '0726', '0918', '1314', '1253', '1177', '0750', '0330', '1010', '0193', '0781', '0872', '1299', '0629', '1055', '0695', '0309', '0467', '0802', '0644', '0461', '0415', '0635', '0924', '0482', '1214', '1076', '0573', '0339', '0833', '0171', '0446', '0385', '1336', '0796', '0369', '0756', '1238', '0674', '0903', '0955', '0650', '0772', '0832', '0766', '0513', '0308', '0857', '0720', '1162', '0848', '0202', '0019', '0071', '1138', '1259', '0522', '0390', '1254', '0360', '0933', '1025', '0512', '1000', '1153', '0842', '0400', '1351', '0751', '0628', '0537', '0583', '0878', '1320', '0491', '0552', '0421', '1315', '1015', '0954', '0040', '1275', '0132', '0812', '0598', '0985', '1016', '1330', '0324', '0589', '1132', '0909', '0049', '0370', '0263', '0735', '0613', '0095', '0431', '0345', '1071', '0689', '1274', '1376', '1345', '0811', '0004', '0888', '0452', '0065', '0964', '0217', '1208', '0893', '0147', '1360', '0939', '1092', '1229', '0110', '1375', '0406', '0817', '0436', '0863', '0787', '0934', '0574', '0949', '0873', '1070', '0497', '0009', '1366', '0528', '1198', '0025', '0619', '0247', '0010', '1107', '1046', '0162', '1305', '1116', '0604', '0827', '0665', '0430', '0116', '0771', '0208', '0086', '0375', '0186', '0248', '0299', '1381', '0284', '0269', '1086', '1290', '0223', '1284', '0177', '0101', '1077', '0156', '0278', '1269', '0238', '0055'])\n",
      "['04', '02', '03', '01']\n"
     ]
    }
   ],
   "source": [
    "rgts = {}\n",
    "for filepath in ATL06_files:\n",
    "    filename = filepath.split('/')[-1]\n",
    "    rgt = filename.split('_')[3][0:4]\n",
    "    track = filename.split('_')[3][4:6]\n",
    "#     print(rgt,track)\n",
    "    if not rgt in rgts.keys():\n",
    "        rgts[rgt] = []\n",
    "        rgts[rgt].append(track)\n",
    "    else:\n",
    "        rgts[rgt].append(track)\n",
    "\n",
    "\n",
    "# all rgt values in our study are are in rgts.keys()\n",
    "print(rgts.keys())\n",
    "\n",
    "# available tracks for each rgt are in rgts[rgt]; ex.:\n",
    "print(rgts['0848'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atl06_to_dict(filename, beam, field_dict=None, index=None, epsg=None):\n",
    "    \"\"\"\n",
    "        Read selected datasets from an ATL06 file\n",
    "\n",
    "        Input arguments:\n",
    "            filename: ATl06 file to read\n",
    "            beam: a string specifying which beam is to be read (ex: gt1l, gt1r, gt2l, etc)\n",
    "            field_dict: A dictinary describing the fields to be read\n",
    "                    keys give the group names to be read, \n",
    "                    entries are lists of datasets within the groups\n",
    "            index: which entries in each field to read\n",
    "            epsg: an EPSG code specifying a projection (see www.epsg.org).  Good choices are:\n",
    "                for Greenland, 3413 (polar stereographic projection, with Greenland along the Y axis)\n",
    "                for Antarctica, 3031 (polar stereographic projection, centered on the Pouth Pole)\n",
    "        Output argument:\n",
    "            D6: dictionary containing ATL06 data.  Each dataset in \n",
    "                dataset_dict has its own entry in D6.  Each dataset \n",
    "                in D6 contains a numpy array containing the \n",
    "                data\n",
    "    \"\"\"\n",
    "    if field_dict is None:\n",
    "        field_dict={None:['latitude','longitude','h_li', 'atl06_quality_summary'],\\\n",
    "                    'ground_track':['x_atc','y_atc'],\\\n",
    "                    'fit_statistics':['dh_fit_dx', 'dh_fit_dy']}\n",
    "    D={}\n",
    "    # below: file_re = regular expression, it will pull apart the regular expression to get the information from the filename\n",
    "    file_re=re.compile('ATL06_(?P<date>\\d+)_(?P<rgt>\\d\\d\\d\\d)(?P<cycle>\\d\\d)(?P<region>\\d\\d)_(?P<release>\\d\\d\\d)_(?P<version>\\d\\d).h5')\n",
    "    with h5py.File(filename,'r') as h5f:\n",
    "        for key in field_dict:\n",
    "            for ds in field_dict[key]:\n",
    "                if key is not None:\n",
    "                    ds_name=beam+'/land_ice_segments/'+key+'/'+ds\n",
    "                else:\n",
    "                    ds_name=beam+'/land_ice_segments/'+ds\n",
    "                if index is not None:\n",
    "                    D[ds]=np.array(h5f[ds_name][index])\n",
    "                else:\n",
    "                    D[ds]=np.array(h5f[ds_name])\n",
    "                if '_FillValue' in h5f[ds_name].attrs:\n",
    "                    bad_vals=D[ds]==h5f[ds_name].attrs['_FillValue']\n",
    "                    D[ds]=D[ds].astype(float)\n",
    "                    D[ds][bad_vals]=np.NaN\n",
    "        D['data_start_utc'] = h5f['/ancillary_data/data_start_utc'][:]\n",
    "        D['delta_time'] = h5f['/' + beam + '/land_ice_segments/delta_time'][:]\n",
    "        D['segment_id'] = h5f['/' + beam + '/land_ice_segments/segment_id'][:]\n",
    "    if epsg is not None:\n",
    "        xy=np.array(pyproj.proj.Proj(epsg)(D['longitude'], D['latitude']))\n",
    "        D['x']=xy[0,:].reshape(D['latitude'].shape)\n",
    "        D['y']=xy[1,:].reshape(D['latitude'].shape)\n",
    "    temp=file_re.search(filename)\n",
    "    D['rgt']=int(temp['rgt'])\n",
    "    D['cycle']=int(temp['cycle'])\n",
    "    D['beam']=beam\n",
    "    return D\n",
    "\n",
    "# A revised code to plot the elevations of segment midpoints (h_li):\n",
    "def plot_elevation(D6, ind=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot midpoint elevation for each ATL06 segment\n",
    "    \"\"\"\n",
    "    if ind is None:\n",
    "        ind=np.ones_like(D6['h_li'], dtype=bool)\n",
    "    # pull out heights of segment midpoints\n",
    "    h_li = D6['h_li'][ind]\n",
    "    # pull out along track x coordinates of segment midpoints\n",
    "    x_atc = D6['x_atc'][ind]\n",
    "\n",
    "    plt.plot(x_atc, h_li, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over rgts and do the correlation processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, path_to_data, product):\n",
    "    \"\"\" \n",
    "    rgt: repeat ground track number of desired data\n",
    "    smoothing: if true, a centered running avergae filter of smoothing_window_size will be used\n",
    "    smoothing_window_size: how large a smoothing window to use (in meters)\n",
    "    dx: desired spacing \n",
    "    path_to_data: \n",
    "    product: ex., ATL06\n",
    "    \"\"\" \n",
    "    \n",
    "    # hard code these for now:\n",
    "    cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "    beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r'] \n",
    "\n",
    "    ### extract data from all available cycles\n",
    "    x_atc = {}\n",
    "    lats = {}\n",
    "    lons = {}\n",
    "    h_li_raw = {} # unsmoothed data; equally spaced x_atc, still has nans \n",
    "    h_li_raw_NoNans = {} # unsmoothed data; equally spaced x_atc, nans filled with noise\n",
    "    h_li = {} # smoothed data, equally spaced x_atc, nans filled with noise \n",
    "    h_li_diff = {}\n",
    "    times = {}\n",
    "    min_seg_ids = {}\n",
    "    segment_ids = {}\n",
    "\n",
    "    cycles_this_rgt = []\n",
    "    for cycle in cycles: # loop over all available cycles\n",
    "        Di = {}\n",
    "        x_atc[cycle] = {}\n",
    "        lats[cycle] = {}\n",
    "        lons[cycle] = {}\n",
    "        h_li_raw[cycle] = {}\n",
    "        h_li_raw_NoNans[cycle] = {}\n",
    "        h_li[cycle] = {}\n",
    "        h_li_diff[cycle] = {}\n",
    "        times[cycle] = {}\n",
    "        min_seg_ids[cycle] = {}\n",
    "        segment_ids[cycle] = {}\n",
    "\n",
    "\n",
    "        filenames = glob.glob(os.path.join(path_to_data, f'*{product}_*_{rgt}{cycle}*_003*.h5'))\n",
    "        error_count=0\n",
    "\n",
    "\n",
    "        for filename in filenames: # try and load any available files; hopefully is just one\n",
    "            try:\n",
    "                for beam in beams:\n",
    "                    Di[filename]=atl06_to_dict(filename,'/'+ beam, index=None, epsg=3031)\n",
    "\n",
    "                    times[cycle][beam] = Di[filename]['data_start_utc']\n",
    "\n",
    "                    # extract h_li and x_atc, and lat/lons for that section                \n",
    "                    x_atc_tmp = Di[filename]['x_atc']\n",
    "                    h_li_tmp = Di[filename]['h_li']#[ixs]\n",
    "                    lats_tmp = Di[filename]['latitude']\n",
    "                    lons_tmp = Di[filename]['longitude']\n",
    "\n",
    "\n",
    "                    # segment ids:\n",
    "                    seg_ids = Di[filename]['segment_id']\n",
    "                    min_seg_ids[cycle][beam] = seg_ids[0]\n",
    "                    #print(len(seg_ids), len(x_atc_tmp))\n",
    "\n",
    "                    # make a monotonically increasing x vector\n",
    "                    # assumes dx = 20 exactly, so be carefull referencing back\n",
    "                    ind = seg_ids - np.nanmin(seg_ids) # indices starting at zero, using the segment_id field, so any skipped segment will be kept in correct location\n",
    "                    x_full = np.arange(np.max(ind)+1) * 20 + x_atc_tmp[0]\n",
    "                    h_full = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                    h_full[ind] = h_li_tmp\n",
    "                    lats_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    lats_full[ind] = lats_tmp\n",
    "                    lons_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    lons_full[ind] = lons_tmp\n",
    "\n",
    "                    ## save the segment id's themselves, with gaps filled in\n",
    "                    segment_ids[cycle][beam] = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                    segment_ids[cycle][beam][ind] = seg_ids\n",
    "\n",
    "\n",
    "                    x_atc[cycle][beam] = x_full\n",
    "                    h_li_raw[cycle][beam] = h_full # preserves nan values\n",
    "                    lons[cycle][beam] = lons_full\n",
    "                    lats[cycle][beam] = lats_full\n",
    "\n",
    "                    ### fill in nans with noise h_li datasets\n",
    "            #                         h = ma.array(h_full,mask =np.isnan(h_full)) # created a masked array, mask is where the nans are\n",
    "            #                         h_full_filled = h.mask * (np.random.randn(*h.shape)) # fill in all the nans with random noise\n",
    "\n",
    "                    ### interpolate nans in pandas\n",
    "                    # put in dataframe for just this step; eventually rewrite to use only dataframes?              \n",
    "                    data = {'x_full': x_full, 'h_full': h_full}\n",
    "                    df = pd.DataFrame(data, columns = ['x_full','h_full'])\n",
    "                    #df.plot(x='x_full',y='h_full')\n",
    "                    # linear interpolation for now\n",
    "                    df['h_full'].interpolate(method = 'linear', inplace = True)\n",
    "                    h_full_interp = df['h_full'].values\n",
    "                    h_li_raw_NoNans[cycle][beam] = h_full_interp # has filled nan values\n",
    "\n",
    "\n",
    "                    # running average smoother /filter\n",
    "                    if smoothing == True:\n",
    "                        h_smoothed = (1/smoothing_window_size) * np.convolve(filt, h_full_interp, mode = 'same')\n",
    "                        h_li[cycle][beam] = h_smoothed\n",
    "\n",
    "                        # differentiate that section of data\n",
    "                        h_diff = (h_smoothed[1:] - h_smoothed[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                    else: \n",
    "                        h_li[cycle][beam] = h_full_interp\n",
    "                        h_diff = (h_full_interp[1:] - h_full_interp[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                    h_li_diff[cycle][beam] = h_diff\n",
    "\n",
    "\n",
    "\n",
    "                    #print(len(x_full), len(h_full), len(lats_full), len(seg_ids), len(h_full_interp), len(h_diff))\n",
    "\n",
    "\n",
    "                cycles_this_rgt+=[cycle]\n",
    "            except KeyError as e:\n",
    "                print(f'file {filename} encountered error {e}')\n",
    "                error_count += 1\n",
    "\n",
    "    print('Cycles available: ' + ','.join(cycles_this_rgt))\n",
    "    return x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, \\\n",
    "            times, min_seg_ids, segment_ids, cycles_this_rgt\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0080, #0 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1131, #1 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0232, #2 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1031, #3 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0634, #4 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0507, #5 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0131, #6 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0192, #7 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0354, #8 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1061, #9 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0492, #10 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190430122344_04920311_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190730080323_04920411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "Cycles available: \n",
      "\n",
      "Processing rgt 0690, #11 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190812071246_06900411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "Cycles available: 03\n",
      "\n",
      "Processing rgt 0970, #12 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0187, #13 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0558, #14 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1335, #15 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0741, #16 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0659, #17 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0894, #18 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1183, #19 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0680, #20 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1101, #21 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1168, #22 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0034, #23 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0568, #24 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0705, #25 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0293, #26 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0711, #27 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1040, #28 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0070, #29 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0543, #30 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1244, #31 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1192, #32 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0314, #33 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0126, #34 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1193, #35 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1147, #36 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0253, #37 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0451, #38 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1122, #39 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0994, #40 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0391, #41 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0141, #42 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0979, #43 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0476, #44 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1223, #45 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1137, #46 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0726, #47 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0918, #48 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1314, #49 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1253, #50 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1177, #51 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0750, #52 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0330, #53 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1010, #54 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0193, #55 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0781, #56 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "rgt 0781 encountered an error\n",
      "negative dimensions are not allowed\n",
      "\n",
      "Processing rgt 0872, #57 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1299, #58 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0629, #59 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1055, #60 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0695, #61 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0309, #62 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0467, #63 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "rgt 0467 encountered an error\n",
      "negative dimensions are not allowed\n",
      "\n",
      "Processing rgt 0802, #64 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0644, #65 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0461, #66 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0415, #67 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0635, #68 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "rgt 0635 encountered an error\n",
      "negative dimensions are not allowed\n",
      "\n",
      "Processing rgt 0924, #69 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0482, #70 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "rgt 0482 encountered an error\n",
      "negative dimensions are not allowed\n",
      "\n",
      "Processing rgt 1214, #71 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1076, #72 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0573, #73 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0339, #74 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0833, #75 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0171, #76 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0446, #77 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0385, #78 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1336, #79 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0796, #80 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0369, #81 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0756, #82 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1238, #83 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0674, #84 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0903, #85 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0955, #86 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0650, #87 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0772, #88 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0832, #89 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190522184208_08320311_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190821142156_08320411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "Cycles available: \n",
      "\n",
      "Processing rgt 0766, #90 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0513, #91 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0308, #92 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0857, #93 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0720, #94 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1162, #95 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0848, #96 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0202, #97 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0019, #98 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0071, #99 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1138, #100 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1259, #101 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0522, #102 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0390, #103 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1254, #104 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0360, #105 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0933, #106 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1025, #107 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0512, #108 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1000, #109 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1153, #110 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0842, #111 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190822060451_08420411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "Cycles available: 03\n",
      "\n",
      "Processing rgt 0400, #112 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1351, #113 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0751, #114 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0628, #115 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190509100707_06280311_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190808054648_06280411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "Cycles available: \n",
      "\n",
      "Processing rgt 0537, #116 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0583, #117 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190506112405_05830311_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "Cycles available: 04\n",
      "\n",
      "Processing rgt 0878, #118 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190824143917_08780411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "Cycles available: 03\n",
      "\n",
      "Processing rgt 1320, #119 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0491, #120 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0552, #121 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0421, #122 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1315, #123 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1015, #124 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0954, #125 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0040, #126 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1275, #127 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0132, #128 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0812, #129 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0598, #130 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0985, #131 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1016, #132 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1330, #133 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0324, #134 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0589, #135 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgt 0589 encountered an error\n",
      "index 95 is out of bounds for axis 0 with size 81\n",
      "\n",
      "Processing rgt 1132, #136 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0909, #137 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0049, #138 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0370, #139 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0263, #140 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0735, #141 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0613, #142 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0095, #143 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0431, #144 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0345, #145 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1071, #146 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0689, #147 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 1274, #148 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1376, #149 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1345, #150 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0811, #151 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0004, #152 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0888, #153 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0452, #154 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0065, #155 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0964, #156 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0217, #157 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1208, #158 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0893, #159 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0147, #160 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1360, #161 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0939, #162 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1092, #163 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1229, #164 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0110, #165 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1375, #166 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0406, #167 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0817, #168 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0436, #169 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0863, #170 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "file /home/jovyan/shared/surface_velocity/FIS_ATL06/processed_ATL06_20190823150456_08630411_003_01.h5 encountered error 'Unable to open object (component not found)'\n",
      "Cycles available: 03\n",
      "\n",
      "Processing rgt 0787, #171 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0934, #172 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0574, #173 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgt 0574 encountered an error\n",
      "index 153 is out of bounds for axis 0 with size 81\n",
      "\n",
      "Processing rgt 0949, #174 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0873, #175 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1070, #176 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0497, #177 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0009, #178 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1366, #179 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0528, #180 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1198, #181 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0025, #182 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0619, #183 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0247, #184 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0010, #185 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1107, #186 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1046, #187 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0162, #188 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1305, #189 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1116, #190 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0604, #191 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:165: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0827, #192 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0665, #193 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "\n",
      "Processing rgt 0430, #194 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0116, #195 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0771, #196 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0208, #197 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0086, #198 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0375, #199 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0186, #200 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0248, #201 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0299, #202 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1381, #203 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0284, #204 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0269, #205 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1086, #206 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1290, #207 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0223, #208 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1284, #209 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0177, #210 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0101, #211 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1077, #212 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0156, #213 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0278, #214 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 1269, #215 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0238, #216 of 218\n",
      "There are 0 files available for this track from cycle 3 onward\n",
      "\n",
      "Processing rgt 0055, #217 of 218\n",
      "There are 1 files available for this track from cycle 3 onward\n",
      "Total number of repeat tracks successfully processed = 60\n"
     ]
    }
   ],
   "source": [
    "cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "# this could be future work\n",
    "\n",
    "beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r']\n",
    "\n",
    "product = 'ATL06'\n",
    "dx = 20 # x_atc coordinate distance\n",
    "\n",
    "# control \n",
    "segment_length = 2000 # m\n",
    "search_width = 800 # m\n",
    "along_track_step = 100 # m; how much to jump between each veloc determination\n",
    "max_percent_nans = 10 # what % of segment length can be nans\n",
    "\n",
    "# smoothing\n",
    "smoothing = True\n",
    "smoothing_window_size = int(np.round(40 / dx)) # meters / dx;\n",
    "# ex., 60 m smoothing window is a 3 point running average smoothed dataset, because each point is 20 m apart\n",
    "filt = np.ones(smoothing_window_size)\n",
    "\n",
    "velocities = {}   \n",
    "correlations = {}     \n",
    "lags = {}\n",
    "x_atcs_for_velocities = {}\n",
    "latitudes = {}\n",
    "longitudes = {}\n",
    "rgts_with_errors = []\n",
    "total_number_repeat_tracks_processed = 0\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if ir >= 0: # in case you want to look at certain ones\n",
    "        try:\n",
    "            print('\\nProcessing rgt ' + rgt + ', #' +str(ir) + ' of ' + str(len(rgts.keys())))\n",
    "\n",
    "            ### load all files for this rgt\n",
    "            rgt_files = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}*_003*.h5'))\n",
    "            n_rgt_files_cycle3_and_after = 0\n",
    "            for file in rgt_files:\n",
    "                if float(file.split('/')[-1].split('_')[3][4:6]) >= 3:\n",
    "                    n_rgt_files_cycle3_and_after += 1\n",
    "\n",
    "            print('There are ' +str(n_rgt_files_cycle3_and_after) + ' files available for this track from cycle 3 onward')\n",
    "\n",
    "\n",
    "            ### only process if there is at least one repeat track during the time period when data overlapped\n",
    "            if n_rgt_files_cycle3_and_after >= 2:\n",
    "\n",
    "\n",
    "                ### extract data from all available cycles\n",
    "                x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, times, min_seg_ids, segment_ids, cycles_this_rgt = \\\n",
    "                    load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, datapath, product)\n",
    "                # 98% sure this code returns the correct values\n",
    "                \n",
    "                ### Determine # of possible velocities:\n",
    "                n_possible_veloc = len(cycles_this_rgt) -1 # naive, for now; can improve later\n",
    "                for veloc_number in range(n_possible_veloc):\n",
    "                    h5_file_out = f'{out_path}rgt{rgt}_veloc{veloc_number}.hdf5'\n",
    "                    with h5py.File(h5_file_out, 'w') as f:\n",
    "                        f['dx'] = dx \n",
    "                        f['product'] = product \n",
    "                        f['segment_length'] = segment_length \n",
    "                        f['search_width'] = search_width \n",
    "                        f['along_track_step'] = along_track_step \n",
    "                        f['max_percent_nans'] = max_percent_nans \n",
    "                        f['smoothing'] = smoothing \n",
    "                        f['smoothing_window_size'] = smoothing_window_size \n",
    "                        f['process_date'] = str(Time.now().value) \n",
    "\n",
    "\n",
    "\n",
    "                    cycle1 = cycles_this_rgt[veloc_number]\n",
    "                    cycle2 = cycles_this_rgt[veloc_number+1]\n",
    "                    t1_string = times[cycle1]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t1 = Time(t1_string)\n",
    "\n",
    "                    t2_string = times[cycle2]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t2 = Time(t2_string)\n",
    "\n",
    "                    dt = (t2 - t1).jd # difference in julian days\n",
    "\n",
    "\n",
    "                    velocities[rgt] = {}   \n",
    "                    correlations[rgt] = {}     \n",
    "                    lags[rgt] = {}\n",
    "\n",
    "                    for beam in beams:\n",
    "                        # fig1, axs = plt.subplots(4,1)\n",
    "\n",
    "\n",
    "                        ### determine x1: larger value for both beams, if different\n",
    "                        min_x_atc_cycle1 = x_atc[cycle1][beam][0]\n",
    "                        min_x_atc_cycle2 = x_atc[cycle2][beam][0]\n",
    "\n",
    "                        # pick out the track that starts at greater x_atc, and use that as x1s vector\n",
    "                        if min_x_atc_cycle1 != min_x_atc_cycle2: \n",
    "                            x1 = np.nanmax([min_x_atc_cycle1,min_x_atc_cycle2])\n",
    "                            cycle_n = np.arange(0,2)[[min_x_atc_cycle1,min_x_atc_cycle2] == x1][0]\n",
    "                            if cycle_n == 0:\n",
    "                                cycletmp = cycle2\n",
    "                            elif cycle_n == 1:\n",
    "                                cycletmp = cycle1\n",
    "                            n_segments_this_track = (len(x_atc[cycletmp][beam]) - search_width/dx) / (along_track_step/dx)\n",
    "                            x1s = x_atc[cycletmp][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "                            # start at search_width/dx in, so the code never tries to get data outside the edges of this rgt\n",
    "                            # add 1 bc the data are differentiated, and h_li_diff is therefore one point shorter\n",
    "\n",
    "                        elif min_x_atc_cycle1 == min_x_atc_cycle2: # doesn't matter which cycle\n",
    "                            x1s = x_atc[cycle1][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "\n",
    "                        ### determine xend: smaller value for both beams, if different\n",
    "                        max_x_atc_cycle1 = x_atc[cycle1][beam][-1]\n",
    "                        max_x_atc_cycle2 = x_atc[cycle2][beam][-1]\n",
    "                        smallest_xatc = np.min([max_x_atc_cycle1,max_x_atc_cycle2])\n",
    "                        ixmax = np.where(x1s >= smallest_xatc - search_width/dx)\n",
    "                        if len(ixmax[0]) >= 1:\n",
    "                            ixtmp = ixmax[0][0]\n",
    "                            x1s = x1s[:ixtmp]\n",
    "\n",
    "                        ### dicts to store info in\n",
    "                        velocities[rgt][beam] = np.empty_like(x1s)\n",
    "                        correlations[rgt][beam] = np.empty_like(x1s)\n",
    "                        lags[rgt][beam] = np.empty_like(x1s)\n",
    "\n",
    "                        midpoints_x_atc = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lat = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lon = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_seg_ids = np.empty(np.shape(x1s)) # for writing out \n",
    "                        \n",
    "                        for xi, x1 in enumerate(x1s):\n",
    "                            # cut out small chunk of data at time t1 (first cycle)\n",
    "                            x_full_t1 = x_atc[cycle1][beam]\n",
    "                            ix_x1 = np.arange(len(x_full_t1))[x_full_t1 >= x1][0]\n",
    "                            ix_x2 = ix_x1 + int(np.round(segment_length/dx))      \n",
    "                            x_t1 = x_full_t1[ix_x1:ix_x2]\n",
    "                            lats_t1 = lats[cycle1][beam][ix_x1:ix_x2]\n",
    "                            lons_t1 = lons[cycle1][beam][ix_x1:ix_x2]\n",
    "                            seg_ids_t1 = segment_ids[cycle1][beam][ix_x1:ix_x2]\n",
    "                            h_li1 = h_li_diff[cycle1][beam][ix_x1-1:ix_x2-1] # start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # find midpoints; this is the position where we will assign the velocity measurement from each window\n",
    "                            n = len(x_t1)\n",
    "                            midpt_ix = int(np.floor(n/2))\n",
    "                            midpoints_x_atc[xi] = x_t1[midpt_ix]\n",
    "                            midpoints_lat[xi] = lats_t1[midpt_ix]\n",
    "                            midpoints_lon[xi] = lons_t1[midpt_ix]\n",
    "                            midpoints_seg_ids[xi] = seg_ids_t1[midpt_ix]\n",
    "                            \n",
    "                            # cut out a wider chunk of data at time t2 (second cycle)\n",
    "                            x_full_t2 = x_atc[cycle2][beam]\n",
    "                            ix_x3 = ix_x1 - int(np.round(search_width/dx)) # offset on earlier end by # indices in search_width\n",
    "                            ix_x4 = ix_x2 + int(np.round(search_width/dx)) # offset on later end by # indices in search_width\n",
    "                            x_t2 = x_full_t2[ix_x3:ix_x4]\n",
    "                            h_li2 = h_li_diff[cycle2][beam][ix_x3-1:ix_x4-1]# start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # plot data\n",
    "                            # axs[0].plot(x_t2, h_li2, 'r')\n",
    "                            # axs[0].plot(x_t1, h_li1, 'k')\n",
    "                            # axs[0].set_xlabel('x_atc (m)')\n",
    "\n",
    "                            ### if there are fewer than 10% nans in either data chunk:\n",
    "                            n_nans1 = np.sum(np.isnan(h_li_raw[cycle1][beam][ix_x1:ix_x2]))\n",
    "                            n_nans2 = np.sum(np.isnan(h_li_raw[cycle2][beam][ix_x3:ix_x4]))\n",
    "\n",
    "                            if (n_nans1 / len(h_li1) <= max_percent_nans/100) and (n_nans2 / len(h_li2) <= max_percent_nans/100):\n",
    "\n",
    "                                # correlate old with newer data\n",
    "                                # detrend both chunks of data\n",
    "                                h_li1 = detrend(h_li1,type = 'linear')\n",
    "                                h_li2 = detrend(h_li2,type = 'linear')\n",
    "\n",
    "                                # normalize both chunks of data\n",
    "            #                         h_li1 = h_li1 / np.nanmax(np.abs(h_li1))\n",
    "            #                         h_li2 = h_li2 / np.nanmax(np.abs(h_li2))\n",
    "\n",
    "                                corr = correlate(h_li1, h_li2, mode = 'valid', method = 'direct') \n",
    "\n",
    "                                # a better way to normalize correlation function: shifting along longer vector\n",
    "                                # normalize by autocorrelations\n",
    "                                coeff_a_val = np.sum(h_li1**2)\n",
    "                                coeff_b_val = np.zeros(len(h_li2) - len(h_li1)+1)\n",
    "                                for shift in range(len(h_li2) - len(h_li1)+1):\n",
    "                                    h_li2_section = h_li2[shift:shift + len(h_li1)]\n",
    "                                    coeff_b_val[shift] = np.sum(h_li2_section **2)\n",
    "                                norm_vec = np.sqrt(coeff_a_val * coeff_b_val)\n",
    "                                corr_normed = corr / np.flip(norm_vec) # i don't really understand why this has to flip, but it does\n",
    "\n",
    "                                lagvec = np.arange(- int(np.round(search_width/dx)), int(search_width/dx) +1,1)# for mode = 'valid'\n",
    "\n",
    "                                shift_vec = lagvec * dx\n",
    "\n",
    "                                ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "                                best_lag = lagvec[ix_peak]\n",
    "                                best_shift = shift_vec[ix_peak]\n",
    "                                velocities[rgt][beam][xi] = best_shift/(dt/365)\n",
    "                                correlations[rgt][beam][xi] = corr_normed[ix_peak]\n",
    "                                lags[rgt][beam][xi] = lagvec[ix_peak]\n",
    "                            else:\n",
    "                                velocities[rgt][beam][xi] = np.nan\n",
    "                                correlations[rgt][beam][xi] = np.nan\n",
    "                                lags[rgt][beam][xi] = np.nan\n",
    "                                \n",
    "                                \n",
    "                        ### Add velocities to hdf5 file for each beam\n",
    "                        with h5py.File(h5_file_out, 'a') as f:\n",
    "                            f[beam +'/x_atc'] = midpoints_x_atc # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/latitudes'] = midpoints_lat # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/longitudes'] = midpoints_lon # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/velocities'] = velocities[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/correlation_coefficients'] = correlations[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/best_lags'] = lags[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/segment_ids'] = midpoints_seg_ids\n",
    "                            f[beam +'/first_cycle_time'] = str(Time(times[cycle1][beam][0]))\n",
    "                            f[beam +'/second_cycle_time'] = str(Time(times[cycle2][beam][0]))\n",
    "\n",
    "                        \n",
    "                    with h5py.File(h5_file_out, 'a') as f:\n",
    "                        f['contributing_cycles'] = ','.join([cycle1,cycle2])\n",
    "\n",
    "            \n",
    "                total_number_repeat_tracks_processed += 1\n",
    "                \n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f'rgt {rgt} encountered an error')\n",
    "            print(e)\n",
    "            rgts_with_errors.append(rgt)\n",
    "            \n",
    "print(f'Total number of repeat tracks successfully processed = {total_number_repeat_tracks_processed}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load data, make a map of correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgt0446_veloc0.hdf5  rgt0598_veloc0.hdf5  rgt0741_veloc0.hdf5\n",
      "rgt0451_veloc0.hdf5  rgt0604_veloc0.hdf5  rgt0750_veloc0.hdf5\n",
      "rgt0452_veloc0.hdf5  rgt0613_veloc0.hdf5  rgt0751_veloc0.hdf5\n",
      "rgt0461_veloc0.hdf5  rgt0619_veloc0.hdf5  rgt0756_veloc0.hdf5\n",
      "rgt0467_veloc0.hdf5  rgt0629_veloc0.hdf5  rgt0766_veloc0.hdf5\n",
      "rgt0476_veloc0.hdf5  rgt0634_veloc0.hdf5  rgt0772_veloc0.hdf5\n",
      "rgt0482_veloc0.hdf5  rgt0635_veloc0.hdf5  rgt0781_veloc0.hdf5\n",
      "rgt0491_veloc0.hdf5  rgt0644_veloc0.hdf5  rgt0787_veloc0.hdf5\n",
      "rgt0497_veloc0.hdf5  rgt0650_veloc0.hdf5  rgt0796_veloc0.hdf5\n",
      "rgt0507_veloc0.hdf5  rgt0659_veloc0.hdf5  rgt0802_veloc0.hdf5\n",
      "rgt0512_veloc0.hdf5  rgt0665_veloc0.hdf5  rgt0811_veloc0.hdf5\n",
      "rgt0513_veloc0.hdf5  rgt0674_veloc0.hdf5  rgt0817_veloc0.hdf5\n",
      "rgt0522_veloc0.hdf5  rgt0680_veloc0.hdf5  rgt0833_veloc0.hdf5\n",
      "rgt0537_veloc0.hdf5  rgt0689_veloc0.hdf5  rgt0848_veloc0.hdf5\n",
      "rgt0552_veloc0.hdf5  rgt0695_veloc0.hdf5  rgt0872_veloc0.hdf5\n",
      "rgt0558_veloc0.hdf5  rgt0705_veloc0.hdf5  rgt0888_veloc0.hdf5\n",
      "rgt0568_veloc0.hdf5  rgt0711_veloc0.hdf5  rgt0893_veloc0.hdf5\n",
      "rgt0573_veloc0.hdf5  rgt0720_veloc0.hdf5  rgt0894_veloc0.hdf5\n",
      "rgt0574_veloc0.hdf5  rgt0726_veloc0.hdf5\n",
      "rgt0589_veloc0.hdf5  rgt0735_veloc0.hdf5\n"
     ]
    }
   ],
   "source": [
    "!ls /home/jovyan/shared/surface_velocity/ATL06_out/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\n",
      "/along_track_step        Dataset {SCALAR}\n",
      "/dx                      Dataset {SCALAR}\n",
      "/gt1l                    Group\n",
      "/gt1l/best_lags          Dataset {397}\n",
      "/gt1l/correlation_coefficients Dataset {397}\n",
      "/gt1l/first_cycle_time   Dataset {SCALAR}\n",
      "/gt1l/latitudes          Dataset {397}\n",
      "/gt1l/longitudes         Dataset {397}\n",
      "/gt1l/second_cycle_time  Dataset {SCALAR}\n",
      "/gt1l/segment_ids        Dataset {397}\n",
      "/gt1l/velocities         Dataset {397}\n",
      "/gt1l/x_atc              Dataset {397}\n",
      "/gt1r                    Group\n",
      "/gt1r/best_lags          Dataset {397}\n",
      "/gt1r/correlation_coefficients Dataset {397}\n",
      "/gt1r/first_cycle_time   Dataset {SCALAR}\n",
      "/gt1r/latitudes          Dataset {397}\n",
      "/gt1r/longitudes         Dataset {397}\n",
      "/gt1r/second_cycle_time  Dataset {SCALAR}\n",
      "/gt1r/segment_ids        Dataset {397}\n",
      "/gt1r/velocities         Dataset {397}\n",
      "/gt1r/x_atc              Dataset {397}\n",
      "/gt2l                    Group\n",
      "/gt2l/best_lags          Dataset {304}\n",
      "/gt2l/correlation_coefficients Dataset {304}\n",
      "/gt2l/first_cycle_time   Dataset {SCALAR}\n",
      "/gt2l/latitudes          Dataset {304}\n",
      "/gt2l/longitudes         Dataset {304}\n",
      "/gt2l/second_cycle_time  Dataset {SCALAR}\n",
      "/gt2l/segment_ids        Dataset {304}\n",
      "/gt2l/velocities         Dataset {304}\n",
      "/gt2l/x_atc              Dataset {304}\n",
      "/gt2r                    Group\n",
      "/gt2r/best_lags          Dataset {304}\n",
      "/gt2r/correlation_coefficients Dataset {304}\n",
      "/gt2r/first_cycle_time   Dataset {SCALAR}\n",
      "/gt2r/latitudes          Dataset {304}\n",
      "/gt2r/longitudes         Dataset {304}\n",
      "/gt2r/second_cycle_time  Dataset {SCALAR}\n",
      "/gt2r/segment_ids        Dataset {304}\n",
      "/gt2r/velocities         Dataset {304}\n",
      "/gt2r/x_atc              Dataset {304}\n",
      "/max_percent_nans        Dataset {SCALAR}\n",
      "/process_date            Dataset {SCALAR}\n",
      "/product                 Dataset {SCALAR}\n",
      "/search_width            Dataset {SCALAR}\n",
      "/segment_length          Dataset {SCALAR}\n",
      "/smoothing               Dataset {SCALAR}\n",
      "/smoothing_window_size   Dataset {SCALAR}\n"
     ]
    }
   ],
   "source": [
    "!h5ls -r /home/jovyan/shared/surface_velocity/ATL06_out/rgt0589_veloc0.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True, False, False, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False, False,  True,  True,  True,  True,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs > 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fe13087484417d92f903060da32d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['best_lags', 'correlation_coefficients', 'first_cycle_time', 'latitudes', 'longitudes', 'second_cycle_time', 'segment_ids', 'velocities', 'x_atc']>\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(file, 'r')\n",
    "lats = f[f'/{beam}/latitudes'][()]\n",
    "lons = f[f'/{beam}/longitudes'][()]\n",
    "coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xy[0], xy[1], 10, coeffs)\n",
    "# f.close()\n",
    "print(f[f'/{beam}/'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88d351ebfef44eb9b9c798a1d488587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b886d904f274673a042a2aad33e8bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    }
   ],
   "source": [
    "moa_datapath = '/srv/tutorial-data/land_ice_applications/'\n",
    "spatial_extent = np.array([-102, -76, -98, -74.5])\n",
    "spatial_extent = np.array([-65, -86, -55, -81])\n",
    "\n",
    "\n",
    "lat=spatial_extent[[1, 3, 3, 1, 1]]\n",
    "lon=spatial_extent[[2, 2, 0, 0, 2]]\n",
    "# project the coordinates to Antarctic polar stereographic\n",
    "xy=np.array(pyproj.Proj(3031)(lon, lat))\n",
    "# get the bounds of the projected coordinates \n",
    "XR=[np.nanmin(xy[0,:]), np.nanmax(xy[0,:])]\n",
    "YR=[np.nanmin(xy[1,:]), np.nanmax(xy[1,:])]\n",
    "MOA=pc.grid.data().from_geotif(os.path.join(moa_datapath, 'MOA','moa_2009_1km.tif'), bounds=[XR, YR])\n",
    "\n",
    "epsg=3031\n",
    "\n",
    "# show the mosaic:\n",
    "plt.close('all')\n",
    "plt.figure(figsize=[8,8])\n",
    "hax0=plt.gcf().add_subplot(111, aspect='equal')\n",
    "MOA.show(ax=hax0,cmap='gray', clim=[14000, 17000])\n",
    "# hax1=plt.gcf().add_subplot(212, aspect='equal', sharex=hax0, sharey=hax0)\n",
    "# MOA.show(ax=hax1, cmap='gray', clim=[14000, 17000]);\n",
    "plt.title('Correlation Coefficient')\n",
    "\n",
    "results_files = glob.glob(out_path + '/*.hdf5')\n",
    "# plt.figure()\n",
    "for file in results_files:\n",
    "    #print(file)\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "\n",
    "                h = hax0.scatter(xy[0], xy[1], 0.25, coeffs, vmin = 0, vmax = 1)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "plt.colorbar(h)\n",
    "\n",
    "outfile = out_path + 'correlation_coefficient.png'\n",
    "plt.savefig(outfile)\n",
    "\n",
    "\n",
    "# show the mosaic:\n",
    "# plt.close('all')\n",
    "plt.figure(figsize=[8,8])\n",
    "hax2=plt.gcf().add_subplot(111, aspect='equal')\n",
    "MOA.show(ax=hax2,cmap='gray', clim=[14000, 17000])\n",
    "# hax1=plt.gcf().add_subplot(212, aspect='equal', sharex=hax0, sharey=hax0)\n",
    "# MOA.show(ax=hax1, cmap='gray', clim=[14000, 17000]);\n",
    "plt.title('Best lag')\n",
    "\n",
    "results_files = glob.glob(out_path + '/*.hdf5')\n",
    "# plt.figure()\n",
    "for file in results_files:\n",
    "    #print(file)\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                lags = f[f'/{beam}/best_lags'][()]\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "\n",
    "                h = hax2.scatter(xy[0], xy[1], 0.25, lags, vmin = -10, vmax = 10)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "plt.colorbar(h)\n",
    "\n",
    "outfile = out_path + 'best_lag.png'\n",
    "plt.savefig(outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# and masked by correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb0373554a34699918104eab30db5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n",
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    }
   ],
   "source": [
    "# correlation_threshold = 0.65\n",
    "\n",
    "moa_datapath = '/srv/tutorial-data/land_ice_applications/'\n",
    "spatial_extent = np.array([-102, -76, -98, -74.5])\n",
    "spatial_extent = np.array([-65, -86, -55, -81])\n",
    "\n",
    "\n",
    "lat=spatial_extent[[1, 3, 3, 1, 1]]\n",
    "lon=spatial_extent[[2, 2, 0, 0, 2]]\n",
    "# project the coordinates to Antarctic polar stereographic\n",
    "xy=np.array(pyproj.Proj(3031)(lon, lat))\n",
    "# get the bounds of the projected coordinates \n",
    "XR=[np.nanmin(xy[0,:]), np.nanmax(xy[0,:])]\n",
    "YR=[np.nanmin(xy[1,:]), np.nanmax(xy[1,:])]\n",
    "MOA=pc.grid.data().from_geotif(os.path.join(moa_datapath, 'MOA','moa_2009_1km.tif'), bounds=[XR, YR])\n",
    "\n",
    "epsg=3031\n",
    "\n",
    "# show the mosaic:\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=[8,8])\n",
    "hax0=fig.add_subplot(211, aspect='equal')\n",
    "MOA.show(ax=hax0,cmap='gray', clim=[14000, 17000])\n",
    "hax1=fig.add_subplot(212, aspect='equal')\n",
    "MOA.show(ax=hax0,cmap='gray', clim=[14000, 17000])\n",
    "# hax2=fig.add_subplot(311, aspect='equal')\n",
    "# MOA.show(ax=hax0,cmap='gray', clim=[14000, 17000])\n",
    "\n",
    "hax0.set_title('Correlation Coefficient, above correlation threshold ' + str(correlation_threshold))\n",
    "hax1.set_title('Best lag, above correlation threshold ' + str(correlation_threshold))\n",
    "# hax2.set_title('Best velocity, above correlation threshold ' + str(correlation_threshold))\n",
    "\n",
    "\n",
    "results_files = glob.glob(out_path + '/*.hdf5')\n",
    "# plt.figure()\n",
    "for file in results_files:\n",
    "    #print(file)\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                lags = f[f'/{beam}/best_lags'][()]\n",
    "                velocs = f[f'/{beam}/velocities'][()]\n",
    "\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "                ixs = coeffs > correlation_threshold\n",
    "\n",
    "                h0 = hax0.scatter(xy[0][ixs], xy[1][ixs], 0.25, coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                h1 = hax1.scatter(xy[0][ixs], xy[1][ixs], 0.25, lags[ixs], vmin = -7, vmax = 7)\n",
    "#                 h2 = hax2.scatter(xy[0][ixs], xy[1][ixs], 0.25, velocs[ixs], vmin = -1000, vmax = 1000)\n",
    "            except:\n",
    "                pass\n",
    "fig.colorbar(h0, ax = hax0)\n",
    "fig.colorbar(h1, ax = hax1)\n",
    "\n",
    "\n",
    "outfile = out_path + 'results_masked.png'\n",
    "plt.savefig(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0894, #18 of 218\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "rgt 0894 encountered an error\n",
      "Total number of repeat tracks successfully processed = 0\n"
     ]
    }
   ],
   "source": [
    "cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "# this could be future work\n",
    "\n",
    "beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r']\n",
    "\n",
    "# try and smooth without filling nans\n",
    "dx = 20 # x_atc coordinate distance\n",
    "smoothing_window_size = int(np.round(40 / dx)) # meters / dx;\n",
    "# ex., 60 m smoothing window is a 3 point running average smoothed dataset, because each point is 20 m apart\n",
    "filt = np.ones(smoothing_window_size)\n",
    "smoothed = True\n",
    "\n",
    "segment_length = 2000 # m\n",
    "search_width = 800 # m\n",
    "\n",
    "along_track_step = 100 # m; how much to jump between each veloc determination\n",
    "\n",
    "max_percent_nans = 10 # what % of segment length can be nans\n",
    "\n",
    "velocities = {}   \n",
    "correlations = {}     \n",
    "lags = {}\n",
    "x_atcs_for_velocities = {}\n",
    "latitudes = {}\n",
    "longitudes = {}\n",
    "rgts_with_errors = []\n",
    "total_number_repeat_tracks_processed = 0\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if ir == 18: # just process a few for the moment\n",
    "        try:\n",
    "            print('\\nProcessing rgt ' + rgt + ', #' +str(ir) + ' of ' + str(len(rgts.keys())))\n",
    "\n",
    "            ### load all files for this rgt\n",
    "            rgt_files = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}*_003*.h5'))\n",
    "            n_rgt_files_cycle3_and_after = 0\n",
    "            for file in rgt_files:\n",
    "                if float(file.split('/')[-1].split('_')[3][4:6]) >= 3:\n",
    "                    n_rgt_files_cycle3_and_after += 1\n",
    "\n",
    "            print('There are ' +str(n_rgt_files_cycle3_and_after) + ' files available for this track from cycle 3 onward')\n",
    "\n",
    "\n",
    "            ### only process if there is at least one repeat track during the time period when data overlapped\n",
    "            if n_rgt_files_cycle3_and_after >= 2:\n",
    "                ### extract data from all available cycles\n",
    "                x_atc = {}\n",
    "                lats = {}\n",
    "                lons = {}\n",
    "                h_li_raw = {} # unsmoothed data; equally spaced x_atc, still has nans \n",
    "                h_li_raw_NoNans = {} # unsmoothed data; equally spaced x_atc, nans filled with noise\n",
    "                h_li = {} # smoothed data, equally spaced x_atc, nans filled with noise \n",
    "                h_li_diff = {}\n",
    "                times = {}\n",
    "                min_seg_ids = {}\n",
    "                segment_ids = {}\n",
    "\n",
    "\n",
    "                cycles_this_rgt = []\n",
    "                for cycle in cycles:\n",
    "                    # load data that matches cycle; put into dictionaries to use shortly\n",
    "                    Di = {}\n",
    "                    x_atc[cycle] = {}\n",
    "                    lats[cycle] = {}\n",
    "                    lons[cycle] = {}\n",
    "                    h_li_raw[cycle] = {}\n",
    "                    h_li_raw_NoNans[cycle] = {}\n",
    "                    h_li[cycle] = {}\n",
    "                    h_li_diff[cycle] = {}\n",
    "                    times[cycle] = {}\n",
    "                    min_seg_ids[cycle] = {}\n",
    "                    segment_ids[cycle] = {}\n",
    "\n",
    "                    filenames = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}{cycle}*_003*.h5'))\n",
    "                    #print(filenames)\n",
    "                    error_count=0\n",
    "                    for filename in filenames:\n",
    "                        try:\n",
    "                            for beam in beams:\n",
    "                                Di[filename]=atl06_to_dict(filename,'/'+ beam, index=None, epsg=3031)\n",
    "\n",
    "                                times[cycle][beam] = Di[filename]['data_start_utc']\n",
    "\n",
    "                                # extract h_li and x_atc, and lat/lons for that section                \n",
    "                                x_atc_tmp = Di[filename]['x_atc']\n",
    "                                h_li_tmp = Di[filename]['h_li']#[ixs]\n",
    "                                lats_tmp = Di[filename]['latitude']\n",
    "                                lons_tmp = Di[filename]['longitude']\n",
    "\n",
    "\n",
    "                                # segment ids:\n",
    "                                seg_ids = Di[filename]['segment_id']\n",
    "                                min_seg_ids[cycle][beam] = seg_ids[0]\n",
    "                                #print(len(seg_ids), len(x_atc_tmp))\n",
    "\n",
    "                                # make a monotonically increasing x vector\n",
    "                                # assumes dx = 20 exactly, so be carefull referencing back\n",
    "                                ind = seg_ids - np.nanmin(seg_ids) # indices starting at zero, using the segment_id field, so any skipped segment will be kept in correct location\n",
    "                                x_full = np.arange(np.max(ind)+1) * 20 + x_atc_tmp[0]\n",
    "                                h_full = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                                h_full[ind] = h_li_tmp\n",
    "                                lats_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                                lats_full[ind] = lats_tmp\n",
    "                                lons_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                                lons_full[ind] = lons_tmp\n",
    "                                \n",
    "                                ## save the segment id's themselves, with gaps filled in\n",
    "                                segment_ids[cycle][beam] = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                                segment_ids[cycle][beam][ind] = seg_ids\n",
    "\n",
    "                                \n",
    "                                x_atc[cycle][beam] = x_full\n",
    "                                h_li_raw[cycle][beam] = h_full # preserves nan values\n",
    "                                lons[cycle][beam] = lons_full\n",
    "                                lats[cycle][beam] = lats_full\n",
    "\n",
    "                                ### fill in nans with noise h_li datasets\n",
    "            #                         h = ma.array(h_full,mask =np.isnan(h_full)) # created a masked array, mask is where the nans are\n",
    "            #                         h_full_filled = h.mask * (np.random.randn(*h.shape)) # fill in all the nans with random noise\n",
    "\n",
    "                                ### interpolate nans in pandas\n",
    "                                # put in dataframe for just this step; eventually rewrite to use only dataframes?\n",
    "                                data = {'x_full': x_full, 'h_full': h_full}\n",
    "                                df = pd.DataFrame(data, columns = ['x_full','h_full'])\n",
    "                                #df.plot(x='x_full',y='h_full')\n",
    "                                # linear interpolation for now\n",
    "                                df['h_full'].interpolate(method = 'linear', inplace = True)\n",
    "                                h_full_interp = df['h_full'].values\n",
    "                                h_li_raw_NoNans[cycle][beam] = h_full_interp # has filled nan values\n",
    "\n",
    "\n",
    "                                # running average smoother /filter\n",
    "                                if smoothed == True:\n",
    "                                    h_smoothed = (1/smoothing_window_size) * np.convolve(filt, h_full_interp, mode = 'same')\n",
    "                                    h_li[cycle][beam] = h_smoothed\n",
    "\n",
    "                                    # differentiate that section of data\n",
    "                                    h_diff = (h_smoothed[1:] - h_smoothed[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                                else: \n",
    "                                    h_li[cycle][beam] = h_full_interp\n",
    "                                    h_diff = (h_full_interp[1:] - h_full_interp[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                                h_li_diff[cycle][beam] = h_diff\n",
    "\n",
    "                            cycles_this_rgt+=[cycle]\n",
    "\n",
    "\n",
    "                        except KeyError as e:\n",
    "                            print(f'file {filename} encountered error {e}')\n",
    "                            error_count += 1\n",
    "\n",
    "                    #print(f\"For rgt {rgt} cycle {cycle}, read {len(Di)} data files of which {error_count} gave errors\")\n",
    "\n",
    "\n",
    "                ### Determine # of possible velocities:\n",
    "                n_possible_veloc = len(cycles_this_rgt) -1 # naive, for now; can improve later\n",
    "                for veloc_number in range(n_possible_veloc):\n",
    "                    cycle1 = cycles_this_rgt[veloc_number]\n",
    "                    cycle2 = cycles_this_rgt[veloc_number+1]\n",
    "                    t1_string = times[cycle1]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t1 = Time(t1_string)\n",
    "\n",
    "                    t2_string = times[cycle2]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t2 = Time(t2_string)\n",
    "\n",
    "                    dt = (t2 - t1).jd # difference in julian days\n",
    "\n",
    "\n",
    "                    velocities[rgt] = {}   \n",
    "                    correlations[rgt] = {}     \n",
    "                    lags[rgt] = {}\n",
    "\n",
    "                    for beam in beams:\n",
    "                        # fig1, axs = plt.subplots(4,1)\n",
    "\n",
    "\n",
    "                        ### determine x1: larger value for both beams, if different\n",
    "                        min_x_atc_cycle1 = x_atc[cycle1][beam][0]\n",
    "                        min_x_atc_cycle2 = x_atc[cycle2][beam][0]\n",
    "\n",
    "                        # pick out the track that starts at greater x_atc, and use that as x1s vector\n",
    "                        if min_x_atc_cycle1 != min_x_atc_cycle2: \n",
    "                            x1 = np.nanmax([min_x_atc_cycle1,min_x_atc_cycle2])\n",
    "                            cycle_n = np.arange(0,2)[[min_x_atc_cycle1,min_x_atc_cycle2] == x1][0]\n",
    "                            if cycle_n == 0:\n",
    "                                cycletmp = cycle2\n",
    "                            elif cycle_n == 1:\n",
    "                                cycletmp = cycle1\n",
    "                            n_segments_this_track = (len(x_atc[cycletmp][beam]) - search_width/dx) / (along_track_step/dx)\n",
    "                            x1s = x_atc[cycletmp][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "                            # start at search_width/dx in, so the code never tries to get data outside the edges of this rgt\n",
    "                            # add 1 bc the data are differentiated, and h_li_diff is therefore one point shorter\n",
    "\n",
    "                        elif min_x_atc_cycle1 == min_x_atc_cycle2: # doesn't matter which cycle\n",
    "                            x1s = x_atc[cycle1][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "\n",
    "                        ### determine xend: smaller value for both beams, if different\n",
    "                        max_x_atc_cycle1 = x_atc[cycle1][beam][-1]\n",
    "                        max_x_atc_cycle2 = x_atc[cycle2][beam][-1]\n",
    "                        smallest_xatc = np.min([max_x_atc_cycle1,max_x_atc_cycle2])\n",
    "                        ixmax = np.where(x1s >= smallest_xatc - search_width/dx)\n",
    "                        if len(ixmax[0]) >= 1:\n",
    "                            ixtmp = ixmax[0][0]\n",
    "                            x1s = x1s[:ixtmp]\n",
    "\n",
    "                        ### dicts to store info in\n",
    "                        velocities[rgt][beam] = np.empty_like(x1s)\n",
    "                        correlations[rgt][beam] = np.empty_like(x1s)\n",
    "                        lags[rgt][beam] = np.empty_like(x1s)\n",
    "\n",
    "                        midpoints_x_atc = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lat = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lon = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_seg_ids = np.empty(np.shape(x1s)) # for writing out \n",
    "                        \n",
    "                        for xi, x1 in enumerate(x1s):\n",
    "                            # cut out small chunk of data at time t1 (first cycle)\n",
    "                            x_full_t1 = x_atc[cycle1][beam]\n",
    "                            ix_x1 = np.arange(len(x_full_t1))[x_full_t1 >= x1][0]\n",
    "                            ix_x2 = ix_x1 + int(np.round(segment_length/dx))      \n",
    "                            x_t1 = x_full_t1[ix_x1:ix_x2]\n",
    "                            lats_t1 = lats[cycle1][beam][ix_x1:ix_x2]\n",
    "                            lons_t1 = lons[cycle1][beam][ix_x1:ix_x2]\n",
    "                            seg_ids_t1 = seg_ids[cycle1][beam][ix_x1:ix_x2]\n",
    "                            h_li1 = h_li_diff[cycle1][beam][ix_x1-1:ix_x2-1] # start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # find midpoints; this is the position where we will assign the velocity measurement from each window\n",
    "                            n = len(x_t1)\n",
    "                            midpt_ix = int(np.floor(n/2))\n",
    "                            midpoints_x_atc[xi] = x_t1[midpt_ix]\n",
    "                            midpoints_lat[xi] = lats_t1[midpt_ix]\n",
    "                            midpoints_lon[xi] = lons_t1[midpt_ix]\n",
    "                            midpoints_seg_ids[xi] = seg_ids_t1[midpt_ix]\n",
    "                            \n",
    "                            # cut out a wider chunk of data at time t2 (second cycle)\n",
    "                            x_full_t2 = x_atc[cycle2][beam]\n",
    "                            ix_x3 = ix_x1 - int(np.round(search_width/dx)) # offset on earlier end by # indices in search_width\n",
    "                            ix_x4 = ix_x2 + int(np.round(search_width/dx)) # offset on later end by # indices in search_width\n",
    "                            x_t2 = x_full_t2[ix_x3:ix_x4]\n",
    "                            h_li2 = h_li_diff[cycle2][beam][ix_x3-1:ix_x4-1]# start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # plot data\n",
    "                            # axs[0].plot(x_t2, h_li2, 'r')\n",
    "                            # axs[0].plot(x_t1, h_li1, 'k')\n",
    "                            # axs[0].set_xlabel('x_atc (m)')\n",
    "\n",
    "                            ### if there are fewer than 10% nans in either data chunk:\n",
    "                            n_nans1 = np.sum(np.isnan(h_li_raw[cycle1][beam][ix_x1:ix_x2]))\n",
    "                            n_nans2 = np.sum(np.isnan(h_li_raw[cycle2][beam][ix_x3:ix_x4]))\n",
    "\n",
    "                            if (n_nans1 / len(h_li1) <= max_percent_nans/100) and (n_nans2 / len(h_li2) <= max_percent_nans/100):\n",
    "\n",
    "                                # correlate old with newer data\n",
    "                                # detrend both chunks of data\n",
    "                                h_li1 = detrend(h_li1,type = 'linear')\n",
    "                                h_li2 = detrend(h_li2,type = 'linear')\n",
    "\n",
    "                                # normalize both chunks of data\n",
    "            #                         h_li1 = h_li1 / np.nanmax(np.abs(h_li1))\n",
    "            #                         h_li2 = h_li2 / np.nanmax(np.abs(h_li2))\n",
    "\n",
    "                                corr = correlate(h_li1, h_li2, mode = 'valid', method = 'direct') \n",
    "\n",
    "                                # a better way to normalize correlation function: shifting along longer vector\n",
    "                                # normalize by autocorrelations\n",
    "                                coeff_a_val = np.sum(h_li1**2)\n",
    "                                coeff_b_val = np.zeros(len(h_li2) - len(h_li1)+1)\n",
    "                                for shift in range(len(h_li2) - len(h_li1)+1):\n",
    "                                    h_li2_section = h_li2[shift:shift + len(h_li1)]\n",
    "                                    coeff_b_val[shift] = np.sum(h_li2_section **2)\n",
    "                                norm_vec = np.sqrt(coeff_a_val * coeff_b_val)\n",
    "                                corr_normed = corr / np.flip(norm_vec) # i don't really understand why this has to flip, but it does\n",
    "\n",
    "\n",
    "                        #         lagvec = np.arange( -(len(h_li1) - 1), len(h_li2), 1)# for mode = 'full'\n",
    "                        #         lagvec = np.arange( -int(search_width/dx) - 1, int(search_width/dx) +1, 1) # for mode = 'valid'\n",
    "                                lagvec = np.arange(- int(np.round(search_width/dx)), int(search_width/dx) +1,1)# for mode = 'valid'\n",
    "\n",
    "                                shift_vec = lagvec * dx\n",
    "\n",
    "                                ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "                                best_lag = lagvec[ix_peak]\n",
    "                                best_shift = shift_vec[ix_peak]\n",
    "                                velocities[rgt][beam][xi] = best_shift/(dt/365)\n",
    "                                correlations[rgt][beam][xi] = corr_normed[ix_peak]\n",
    "                                lags[rgt][beam][xi] = lagvec[ix_peak]\n",
    "                            else:\n",
    "                                velocities[rgt][beam][xi] = np.nan\n",
    "                                correlations[rgt][beam][xi] = np.nan\n",
    "                                lags[rgt][beam][xi] = np.nan\n",
    "                                \n",
    "                                \n",
    "                        ### Add velocities to hdf5 file for each beam\n",
    "                        h5_file_out = f'{out_path}rgt{rgt}.hdf5'\n",
    "                        with h5py.File(h5_file_out, 'w') as f:\n",
    "                            f[beam +'/x_atc'] = midpoints_x_atc # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/latitudes'] = midpoints_lat # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/longitudes'] = midpoints_lon # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/velocities'] = velocities[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/correlation_coefficients'] = correlations[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/best_lags'] = lags[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/segment_ids'] = midpoints_seg_ids\n",
    "                            \n",
    "#                         f'{out_path}rgt{rgt}_{beam}.txt'\n",
    "#                         f = open(file_out,'w')\n",
    "\n",
    "\n",
    "#                         header0 = 'segment_length='+str(segment_length)+',segment_step='+str((dx))+'m,search_width='+str(search_width) + 'm'\n",
    "#                         header = 'x_atc_segment_middle'\n",
    "#                         for beam in beams:\n",
    "#                             header = header + ',' + beam + '_veloc,' + beam + '_correlationValue'\n",
    "#                         f.write(header0 + '\\n')\n",
    "#                         f.write(header + '\\n')\n",
    "                \n",
    "                \n",
    "                total_number_repeat_tracks_processed += 1\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "        except (ValueError, IndexError):\n",
    "            print(f'rgt {rgt} encountered an error')\n",
    "            rgts_with_errors.append(rgt)\n",
    "            \n",
    "print(f'Total number of repeat tracks successfully processed = {total_number_repeat_tracks_processed}')\n",
    "\n",
    "                    # axs[1].plot(lagvec,corr)\n",
    "                    # axs[1].plot(lagvec[ix_peak],corr[ix_peak], 'r*')\n",
    "                    # axs[1].set_xlabel('lag (samples)')\n",
    "\n",
    "                    # axs[2].plot(shift_vec,corr)\n",
    "                    # axs[2].plot(shift_vec[ix_peak],corr[ix_peak], 'r*')\n",
    "                    # axs[2].set_xlabel('shift (m)')\n",
    "\n",
    "                    ## plot shifted data\n",
    "                    # axs[3].plot(x_t2, h_li2, 'r')\n",
    "                    # axs[3].plot(x_t1 - best_shift, h_li1, 'k')\n",
    "                    # axs[3].set_xlabel('x_atc (m)')\n",
    "\n",
    "                    # axs[0].text(x_t2[100], 0.6*np.nanmax(h_li2), beam)\n",
    "                    # axs[1].text(lagvec[5], 0.6*np.nanmax(corr), 'best lag: ' + str(best_lag) + '; corr val: ' + str(np.round(corr[ix_peak],3)))\n",
    "                    # axs[2].text(shift_vec[5], 0.6*np.nanmax(corr), 'best shift: ' + str(best_shift) + ' m'+ '; corr val: ' + str(np.round(corr[ix_peak],3)))\n",
    "                    # axs[2].text(shift_vec[5], 0.3*np.nanmax(corr), 'veloc of ' + str(np.round(best_shift/(dt/365),1)) + ' m/yr')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
