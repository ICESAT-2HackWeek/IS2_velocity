{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from icepyx import icesat2data as ipd\n",
    "import os, glob, re, h5py, sys, pyproj\n",
    "import matplotlib as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from astropy.time import Time\n",
    "from scipy.signal import correlate, detrend\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import pointCollection as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0848 Only in this notebook\n",
    "\n",
    "Haven't downloaded the rest of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Where are the data to be processed\n",
    "#datapath = '/home/jovyan/shared/surface_velocity/FIS_ATL06'\n",
    "\n",
    "#local data path\n",
    "# datapath = '/media/rag110/ADATA SD700/ICESat2/download/FIS'\n",
    "\n",
    "map_data_root = '/Users/grace/Dropbox/Cornell/projects/003/FIS_data/'\n",
    "datapath = '/Users/grace/Dropbox/Cornell/projects/003/ATL06_0848/data/'\n",
    "datapath = '/Users/grace/Dropbox/Cornell/projects/003/git_repo/surface_velocity/contributors/grace_barcheck/download/'\n",
    "# datapath = #'/home/jovyan/shared/surface_velocity/FIS_ATL06'\n",
    "ATL06_files=glob.glob(os.path.join(datapath, '*.h5'))\n",
    "# print(ATL06_files)\n",
    "\n",
    "### Where to save the results\n",
    "# out_path = '/home/jovyan/shared/surface_velocity/ATL06_out2/'\n",
    "out_path = '/Users/grace/Dropbox/Cornell/projects/003/ATL06_0848/out/'\n",
    "\n",
    "#local out_path Rodrigo\n",
    "# out_path = '/media/rag110/ADATA SD700/ICESat2/output/FIS/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some necessary scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures_along_track_velocity(x_ps_beam, y_ps_beam , spatial_extent, vel_x_path, vel_y_path):\n",
    "    \"\"\"\n",
    "    \n",
    "    is2_dict: Python dictionary with ATL06 track data\n",
    "    spatial_extent: bounding box of the interest area in the format:\n",
    "                    (e.g. [-65, -86, -55, -81] == [min_lon, min_lat, max_lon, max_lat])\n",
    "    path: local path to velocity data\n",
    "    vel_x: tif velocity raster with x component\n",
    "    vel_y: tif velocity raster with y component\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #fix with if statement about type of list or array DONE\n",
    "    \n",
    "    if type(spatial_extent) == type([]):\n",
    "    \n",
    "        spatial_extent = np.array([spatial_extent])\n",
    "    \n",
    "        \n",
    "    lat=spatial_extent[[1, 3, 3, 1, 1]]\n",
    "    lon=spatial_extent[[2, 2, 0, 0, 2]]\n",
    "\n",
    "    # project the coordinates to Antarctic polar stereographic\n",
    "    xy=np.array(pyproj.Proj(3031)(lon, lat))\n",
    "    # get the bounds of the projected coordinates \n",
    "    XR=[np.nanmin(xy[0,:]), np.nanmax(xy[0,:])]\n",
    "    YR=[np.nanmin(xy[1,:]), np.nanmax(xy[1,:])]\n",
    "    \n",
    "    #Measures_vx=pc.grid.data().from_geotif(os.path.join(data_root,vel_x), bounds=[XR, YR])\n",
    "    #Measures_vy=pc.grid.data().from_geotif(os.path.join(data_root,vel_y), bounds=[XR, YR])\n",
    "    \n",
    "    Measures_vx=pc.grid.data().from_geotif(vel_x_path, bounds=[XR, YR])\n",
    "    Measures_vy=pc.grid.data().from_geotif(vel_y_path, bounds=[XR, YR])\n",
    "    \n",
    "    vx = Measures_vx.interp(x_ps_beam,y_ps_beam)\n",
    "    vy = Measures_vy.interp(x_ps_beam,y_ps_beam)\n",
    "\n",
    "    #Solve for angle to rotate Vy to be along track and Vx to be across track\n",
    "    import math\n",
    "    xL=abs((x_ps_beam[0])-(x_ps_beam[1]))\n",
    "    yL=abs((y_ps_beam[0])-(y_ps_beam[1]))\n",
    "\n",
    "    #decides if is descending or ascending path\n",
    "    if x_ps_beam[0]-x_ps_beam[1] < 0:\n",
    "\n",
    "        theta_rad=math.atan(xL/yL)\n",
    "        #theta_deg=theta_rad*180/math.pi\n",
    "        v_along=vy/math.cos(theta_rad)\n",
    "        #v_across=vx/math.cos(theta_rad)\n",
    "\n",
    "    else:\n",
    "    \n",
    "        theta_rad=math.atan(xL/yL)\n",
    "        #theta_deg=theta_rad*180/math.pi\n",
    "        v_along=vy/math.sin(theta_rad)\n",
    "        #v_across=vx/math.sin(theta_rad)\n",
    "    \n",
    "    #Vdiff=vy-v_along\n",
    "    return v_along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Revised version of code from Ben Smith to read in the hdf5 files and extract necessary datasets and information\n",
    "def atl06_to_dict(filename, beam, field_dict=None, index=None, epsg=None):\n",
    "    \"\"\"\n",
    "        Read selected datasets from an ATL06 file\n",
    "\n",
    "        Input arguments:\n",
    "            filename: ATl06 file to read\n",
    "            beam: a string specifying which beam is to be read (ex: gt1l, gt1r, gt2l, etc)\n",
    "            field_dict: A dictinary describing the fields to be read\n",
    "                    keys give the group names to be read, \n",
    "                    entries are lists of datasets within the groups\n",
    "            index: which entries in each field to read\n",
    "            epsg: an EPSG code specifying a projection (see www.epsg.org).  Good choices are:\n",
    "                for Greenland, 3413 (polar stereographic projection, with Greenland along the Y axis)\n",
    "                for Antarctica, 3031 (polar stereographic projection, centered on the Pouth Pole)\n",
    "        Output argument:\n",
    "            D6: dictionary containing ATL06 data.  Each dataset in \n",
    "                dataset_dict has its own entry in D6.  Each dataset \n",
    "                in D6 contains a numpy array containing the \n",
    "                data\n",
    "    \"\"\"\n",
    "    if field_dict is None:\n",
    "        field_dict={None:['latitude','longitude','h_li', 'atl06_quality_summary'],\\\n",
    "                    'ground_track':['x_atc','y_atc'],\\\n",
    "                    'fit_statistics':['dh_fit_dx', 'dh_fit_dy']}\n",
    "    D={}\n",
    "    # below: file_re = regular expression, it will pull apart the regular expression to get the information from the filename\n",
    "    file_re=re.compile('ATL06_(?P<date>\\d+)_(?P<rgt>\\d\\d\\d\\d)(?P<cycle>\\d\\d)(?P<region>\\d\\d)_(?P<release>\\d\\d\\d)_(?P<version>\\d\\d).h5')\n",
    "    with h5py.File(filename,'r') as h5f:\n",
    "        for key in field_dict:\n",
    "            for ds in field_dict[key]:\n",
    "                if key is not None:\n",
    "                    ds_name=beam+'/land_ice_segments/'+key+'/'+ds\n",
    "                else:\n",
    "                    ds_name=beam+'/land_ice_segments/'+ds\n",
    "                if index is not None:\n",
    "                    D[ds]=np.array(h5f[ds_name][index])\n",
    "                else:\n",
    "                    D[ds]=np.array(h5f[ds_name])\n",
    "                if '_FillValue' in h5f[ds_name].attrs:\n",
    "                    bad_vals=D[ds]==h5f[ds_name].attrs['_FillValue']\n",
    "                    D[ds]=D[ds].astype(float)\n",
    "                    D[ds][bad_vals]=np.NaN\n",
    "        D['data_start_utc'] = h5f['/ancillary_data/data_start_utc'][:]\n",
    "        D['delta_time'] = h5f['/' + beam + '/land_ice_segments/delta_time'][:]\n",
    "        D['segment_id'] = h5f['/' + beam + '/land_ice_segments/segment_id'][:]\n",
    "    if epsg is not None:\n",
    "        xy=np.array(pyproj.proj.Proj(epsg)(D['longitude'], D['latitude']))\n",
    "        D['x']=xy[0,:].reshape(D['latitude'].shape)\n",
    "        D['y']=xy[1,:].reshape(D['latitude'].shape)\n",
    "    temp=file_re.search(filename)\n",
    "    D['rgt']=int(temp['rgt'])\n",
    "    D['cycle']=int(temp['cycle'])\n",
    "    D['beam']=beam\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some functions\n",
    "# MISSING HERE: mask by data quality?\n",
    "def load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, path_to_data, product):\n",
    "    \"\"\" \n",
    "    rgt: repeat ground track number of desired data\n",
    "    smoothing: if true, a centered running avergae filter of smoothing_window_size will be used\n",
    "    smoothing_window_size: how large a smoothing window to use (in meters)\n",
    "    dx: desired spacing \n",
    "    path_to_data: \n",
    "    product: ex., ATL06\n",
    "    \"\"\" \n",
    "    \n",
    "    # hard code these for now:\n",
    "    cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly\n",
    "    beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r'] \n",
    "\n",
    "    ### extract data from all available cycles\n",
    "    x_atc = {}\n",
    "    lats = {}\n",
    "    lons = {}\n",
    "    h_li_raw = {} # unsmoothed data; equally spaced x_atc, still has nans \n",
    "    h_li_raw_NoNans = {} # unsmoothed data; equally spaced x_atc, nans filled with noise\n",
    "    h_li = {} # smoothed data, equally spaced x_atc, nans filled with noise \n",
    "    h_li_diff = {}\n",
    "    times = {}\n",
    "    min_seg_ids = {}\n",
    "    segment_ids = {}\n",
    "    x_ps= {}\n",
    "    y_ps= {}\n",
    "\n",
    "    cycles_this_rgt = []\n",
    "    for cycle in cycles: # loop over all available cycles\n",
    "        Di = {}\n",
    "        x_atc[cycle] = {}\n",
    "        lats[cycle] = {}\n",
    "        lons[cycle] = {}\n",
    "        h_li_raw[cycle] = {}\n",
    "        h_li_raw_NoNans[cycle] = {}\n",
    "        h_li[cycle] = {}\n",
    "        h_li_diff[cycle] = {}\n",
    "        times[cycle] = {}\n",
    "        min_seg_ids[cycle] = {}\n",
    "        segment_ids[cycle] = {}\n",
    "        x_ps[cycle]= {}\n",
    "        y_ps[cycle]= {}\n",
    "\n",
    "\n",
    "        filenames = glob.glob(os.path.join(path_to_data, f'*{product}_*_{rgt}{cycle}*_003*.h5'))\n",
    "        error_count=0\n",
    "\n",
    "\n",
    "        for filename in filenames: # try and load any available files; hopefully is just one\n",
    "            try:\n",
    "                for beam in beams:\n",
    "                    Di[filename]=atl06_to_dict(filename,'/'+ beam, index=None, epsg=3031)\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    times[cycle][beam] = Di[filename]['data_start_utc']\n",
    "\n",
    "                    # extract h_li and x_atc, and lat/lons for that section                \n",
    "                    x_atc_tmp = Di[filename]['x_atc']\n",
    "                    h_li_tmp = Di[filename]['h_li']#[ixs]\n",
    "                    lats_tmp = Di[filename]['latitude']\n",
    "                    lons_tmp = Di[filename]['longitude']\n",
    "                    x_ps_tmp = Di[filename]['x']\n",
    "                    y_ps_tmp= Di[filename]['y']\n",
    "\n",
    "\n",
    "                    # segment ids:\n",
    "                    seg_ids = Di[filename]['segment_id']\n",
    "                    min_seg_ids[cycle][beam] = seg_ids[0]\n",
    "                    #print(len(seg_ids), len(x_atc_tmp))\n",
    "\n",
    "                    # make a monotonically increasing x vector\n",
    "                    # assumes dx = 20 exactly, so be carefull referencing back\n",
    "                    ind = seg_ids - np.nanmin(seg_ids) # indices starting at zero, using the segment_id field, so any skipped segment will be kept in correct location\n",
    "                    x_full = np.arange(np.max(ind)+1) * 20 + x_atc_tmp[0]\n",
    "                    h_full = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                    h_full[ind] = h_li_tmp\n",
    "                    lats_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    lats_full[ind] = lats_tmp\n",
    "                    lons_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    lons_full[ind] = lons_tmp\n",
    "                    x_ps_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    x_ps_full[ind] = x_ps_tmp\n",
    "                    y_ps_full = np.zeros(np.shape(x_full)) * np.nan\n",
    "                    y_ps_full[ind] = y_ps_tmp\n",
    "\n",
    "                    ## save the segment id's themselves, with gaps filled in\n",
    "                    segment_ids[cycle][beam] = np.zeros(np.max(ind)+1) + np.NaN\n",
    "                    segment_ids[cycle][beam][ind] = seg_ids\n",
    "\n",
    "\n",
    "                    x_atc[cycle][beam] = x_full\n",
    "                    h_li_raw[cycle][beam] = h_full # preserves nan values\n",
    "                    lons[cycle][beam] = lons_full\n",
    "                    lats[cycle][beam] = lats_full\n",
    "                    x_ps[cycle][beam] = x_ps_full\n",
    "                    y_ps[cycle][beam] = y_ps_full\n",
    "\n",
    "                    ### fill in nans with noise h_li datasets\n",
    "            #                         h = ma.array(h_full,mask =np.isnan(h_full)) # created a masked array, mask is where the nans are\n",
    "            #                         h_full_filled = h.mask * (np.random.randn(*h.shape)) # fill in all the nans with random noise\n",
    "\n",
    "                    ### interpolate nans in pandas\n",
    "                    # put in dataframe for just this step; eventually rewrite to use only dataframes?              \n",
    "                    data = {'x_full': x_full, 'h_full': h_full}\n",
    "                    df = pd.DataFrame(data, columns = ['x_full','h_full'])\n",
    "                    #df.plot(x='x_full',y='h_full')\n",
    "                    # linear interpolation for now\n",
    "                    df['h_full'].interpolate(method = 'linear', inplace = True)\n",
    "                    h_full_interp = df['h_full'].values\n",
    "                    h_li_raw_NoNans[cycle][beam] = h_full_interp # has filled nan values\n",
    "\n",
    "\n",
    "                    # running average smoother /filter\n",
    "                    if smoothing == True:\n",
    "                        h_smoothed = (1/smoothing_window_size) * np.convolve(filt, h_full_interp, mode = 'same')\n",
    "                        h_li[cycle][beam] = h_smoothed\n",
    "\n",
    "                        # differentiate that section of data\n",
    "                        h_diff = (h_smoothed[1:] - h_smoothed[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                    else: \n",
    "                        h_li[cycle][beam] = h_full_interp\n",
    "                        h_diff = (h_full_interp[1:] - h_full_interp[0:-1]) / (x_full[1:] - x_full[0:-1])\n",
    "                    h_li_diff[cycle][beam] = h_diff\n",
    "\n",
    "\n",
    "\n",
    "                    #print(len(x_full), len(h_full), len(lats_full), len(seg_ids), len(h_full_interp), len(h_diff))\n",
    "\n",
    "\n",
    "                cycles_this_rgt+=[cycle]\n",
    "            except KeyError as e:\n",
    "                print(f'file {filename} encountered error {e}')\n",
    "                error_count += 1\n",
    "\n",
    "    print('Cycles available: ' + ','.join(cycles_this_rgt))\n",
    "    return x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, \\\n",
    "            times, min_seg_ids, segment_ids, cycles_this_rgt, x_ps, y_ps\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNFINISHED FUNCTION - basic structure is good\n",
    "### working on debugging problems with peaks at edges for the sub-sample routine \n",
    "\n",
    "def find_correlation_peak(lagvec, shift_vec, corr_normed, max_width, min_width, dx_interp, fit_type, plotting):\n",
    "    # max_width = how many SAMPLES wide the half peak can be, max, samples\n",
    "    # min_width = the narrowest the half peak can be, samples\n",
    "    # dx_interp = interpolated sample spacing, in units of SAMPLES\n",
    "    \n",
    "    from scipy.interpolate import interp1d\n",
    "    \n",
    "    # peak of un-interpolated data\n",
    "    ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "    ix_peak = np.where(corr_normed == np.nanmax(corr_normed))[0][0]\n",
    "    \n",
    "    if fit_type == 'raw':\n",
    "        best_lag = lagvec[ix_peak]\n",
    "        best_shift = shift_vec[ix_peak]\n",
    "        peak_corr_value = corr_normed[ix_peak]\n",
    "        \n",
    "    else:\n",
    "        ### Assume we're doing a sub-sample fit\n",
    "        # find troughs before and after peak\n",
    "        \n",
    "        corr_diff = corr_normed[1:] - corr_normed[:-1] # to find troughs, zero-crossings\n",
    "\n",
    "        # cases:\n",
    "        # ix_peak is too close to the start (ix_peak < min_width)\n",
    "            # calculate right width by finding trough or max/min, use ix_peak as left width\n",
    "        if ix_peak < min_width:\n",
    "            left_width = ix_peak # go to left edge\n",
    "            # determine right width\n",
    "            if ix_peak == 0:\n",
    "                ix_next_trough = np.where(corr_diff[ix_peak:] > 0)[0][0] + ix_peak\n",
    "            else:\n",
    "                ix_next_trough = np.where(corr_diff[ix_peak-1:] > 0)[0][0] + ix_peak\n",
    "            width_next_trough = -(ix_peak - ix_next_trough)\n",
    "            if width_next_trough < min_width:\n",
    "                right_width = min_width\n",
    "            elif width_next_trough > max_width:\n",
    "                right_width = max_width\n",
    "            else:\n",
    "                right_width = width_next_trough\n",
    "\n",
    "        # ix_peak is too close to the end (len(corr_normed) - ix_peak < min_width)\n",
    "            # calculate left width by finding trough or max/min, use len(corr_normed) - ix_peak as right width\n",
    "        elif len(corr_normed) - ix_peak < min_width:\n",
    "            right_width = len(corr_normed) - ix_peak -1\n",
    "            #determine left width\n",
    "            ix_prev_trough = ix_peak - np.where(np.flip(corr_diff[:ix_peak-1]) < 0)[0][0]\n",
    "            width_prev_trough = ix_peak - ix_prev_trough\n",
    "            if width_prev_trough < min_width:\n",
    "                left_width = min_width\n",
    "            elif width_prev_trough > max_width:\n",
    "                left_width = max_width\n",
    "            else:\n",
    "                left_width = width_prev_trough\n",
    "\n",
    "        # other\n",
    "        else:\n",
    "            # calculate both left and right width\n",
    "            ix_next_trough = np.where(corr_diff[ix_peak-1:] > 0)[0][0] + ix_peak\n",
    "            ix_prev_trough = ix_peak - np.where(np.flip(corr_diff[:ix_peak-1]) < 0)[0][0]\n",
    "            # if width is greater than min, pick smaller width, so is same on both sides\n",
    "            width = np.min([ix_peak - ix_prev_trough, -(ix_peak - ix_next_trough)])\n",
    "            if width > max_width:\n",
    "                width = max_width\n",
    "            elif width < min_width:\n",
    "                width = min_width\n",
    "            left_width = width\n",
    "            right_width = width\n",
    "            \n",
    "#         # deal with edges; just go to edge\n",
    "#         if ix_peak + width >= len(corr_normed):\n",
    "#             right_width = len(corr_normed) - ix_peak -1\n",
    "#         else:\n",
    "#             right_width = width\n",
    "            \n",
    "#         if width>= ix_peak:\n",
    "#             left_width = ix_peak\n",
    "#         else:\n",
    "#             left_width = width\n",
    "        \n",
    "        # cut out data and an x vector before and after the peak (for plotting)\n",
    "        dcut = corr_normed[ix_peak - left_width: ix_peak + right_width +1]\n",
    "        xvec_cut = np.arange(ix_peak - left_width,  ix_peak + right_width + 1)\n",
    "        lagvec_cut = lagvec[xvec_cut]\n",
    "        \n",
    "        if fit_type == 'parabola':        \n",
    "            # fit parabola; in these data, usually a poor fit\n",
    "            fit_coeffs = np.polyfit(xvec_cut, dcut, deg=2, full = True)\n",
    "\n",
    "            # create xvec to interpolate the parabola onto and compute \n",
    "            interp_xvec = np.arange(ix_peak - left_width,  ix_peak + right_width + 1, 0.01)\n",
    "            fit = np.polyval(p = fit_coeffs[0], x = interp_xvec)\n",
    "\n",
    "            # interp lagvec\n",
    "            lagvec_interp = np.arange(lagvec[ix_peak - left_width], lagvec[ix_peak + right_width + 1], dx_interp)\n",
    "\n",
    "            # compute peak\n",
    "            ix_peak_interp = np.where(fit == np.max(fit))[0][0]\n",
    "            best_lag = interp_xvec[ix_peak_interp]\n",
    "            peak_corr_value = fit[ix_peak_interp]\n",
    "\n",
    "        elif fit_type == 'cubic':\n",
    "#             # find troughs before and after peak\n",
    "#             corr_diff = corr_normed[1:] - corr_normed[:-1]\n",
    "#             ix_next_trough = np.where(corr_diff[ix_peak:] > 0)[0][0] + ix_peak\n",
    "#             ix_prev_trough = ix_peak - np.where(np.flip(corr_diff[:ix_peak]) < 0)[0][0]\n",
    "#             width = np.min([ix_peak - ix_prev_trough, -(ix_peak - ix_next_trough)])\n",
    "#             if width > max_width:\n",
    "#                 width = max_width\n",
    "#             elif width < min_width:\n",
    "#                 width = min_width\n",
    "\n",
    "#             # cut out data and an x vector before and after the peak (for plotting)\n",
    "#             dcut = corr_normed[ix_peak - width: ix_peak + width +1]\n",
    "#             xvec_cut = np.arange(ix_peak - width,  ix_peak + width + 1)\n",
    "#             lagvec_cut = lagvec[xvec_cut]\n",
    "\n",
    "#             if np.sum(np.isnan(dcut)) >0:\n",
    "#                 print(str(np.sum(np.isnan(dcut))) + ' nans')\n",
    "\n",
    "            # cubic spline interpolation\n",
    "            # create xvector to interpolate onto\n",
    "            interp_xvec = np.arange(ix_peak - left_width,  ix_peak + right_width , dx_interp)\n",
    "            # create interpolation function\n",
    "            f = interp1d(xvec_cut, dcut, kind = 'cubic')\n",
    "            # evaluate interpolation function on new xvector\n",
    "            fit = f(interp_xvec)\n",
    "\n",
    "            # interpolate lagvec\n",
    "            lagvec_interp = np.arange(lagvec[ix_peak - left_width], lagvec[ix_peak + right_width], dx_interp)\n",
    "\n",
    "            # compute peak\n",
    "            ix_peak_interp = np.where(fit == np.max(fit))[0][0]\n",
    "            best_lag = lagvec_interp[ix_peak_interp]\n",
    "            peak_corr_value = fit[ix_peak_interp]\n",
    "\n",
    "    if plotting:\n",
    "        plt.figure()\n",
    "        plt.plot(lagvec, corr_normed, 'k-')\n",
    "        plt.xlabel('Lag (samples)')\n",
    "        plt.ylabel('Correlation coefficient')\n",
    "        plt.plot(lagvec[ix_peak], corr_normed[ix_peak],'b.')\n",
    "        plt.text(lagvec[-20], np.max(corr_normed), 'best raw lag: ' + str(lagvec[ix_peak]) + '\\ncorr coeff: ' + str(corr_normed[ix_peak]))\n",
    "\n",
    "\n",
    "        if fit_type != 'raw':\n",
    "            plt.plot(lagvec_cut, dcut, 'b-')\n",
    "            plt.plot(lagvec_interp, fit, 'r--')\n",
    "            plt.plot(lagvec_interp[ix_peak_interp], fit[ix_peak_interp], 'r.')\n",
    "            plt.text(lagvec[-20], 0.7*np.max(corr_normed), 'best interpolated lag: ' + str(best_lag) + '\\ncorr coeff: ' + str(peak_corr_value))\n",
    "\n",
    "    \n",
    "    return best_lag, peak_corr_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_measures_along_track_comparison(rgt, out_path, correlation_threshold, plot_out_location, map_data_root, velocity_number, close = False):\n",
    "    # currently just the first velocity determination, veloc0\n",
    "    # out_path is where the xcorr results are stored\n",
    "    # plot_out_location is where to save the plot\n",
    "    # map_data_root is where the map data are stored, specifically must contain moa_2009_1km.tif for this specific code to work\n",
    "    \n",
    "    file = out_path + 'rgt' + rgt + '_veloc' + str(velocity_number).zfill(2) + '.hdf5' #< eventually, make velocity number two digits\n",
    "\n",
    "    if glob.glob(file):\n",
    "\n",
    "        ### MOA parameters\n",
    "        moa_datapath = map_data_root #'/srv/tutorial-data/land_ice_applications/'\n",
    "        spatial_extent = np.array([-102, -76, -98, -74.5])\n",
    "        spatial_extent = np.array([-65, -86, -55, -81])\n",
    "\n",
    "        lat=spatial_extent[[1, 3, 3, 1, 1]]\n",
    "        lon=spatial_extent[[2, 2, 0, 0, 2]]\n",
    "        # project the coordinates to Antarctic polar stereographic\n",
    "        xy=np.array(pyproj.Proj(3031)(lon, lat))\n",
    "        # get the bounds of the projected coordinates \n",
    "        XR=[np.nanmin(xy[0,:]), np.nanmax(xy[0,:])]\n",
    "        YR=[np.nanmin(xy[1,:]), np.nanmax(xy[1,:])]\n",
    "        MOA=pc.grid.data().from_geotif(os.path.join(moa_datapath, 'moa_2009_1km.tif'), bounds=[XR, YR])\n",
    "    #             MOA=pc.grid.data().from_geotif(os.path.join(moa_datapath, 'MOA','moa_2009_1km.tif'), bounds=[XR, YR])\n",
    "\n",
    "\n",
    "        epsg=3031 #PS?\n",
    "\n",
    "        plt.close('all')\n",
    "        fig = plt.figure(figsize=[11,8])\n",
    "        grid = plt.GridSpec(6, 2, wspace=0.4, hspace=0.3)\n",
    "        haxMOA=fig.add_subplot(grid[0:4,1])\n",
    "        MOA.show(ax=haxMOA,cmap='gray', clim=[14000, 17000])\n",
    "\n",
    "\n",
    "        with h5py.File(file, 'r') as f:\n",
    "                for ib, beam in enumerate(beams):\n",
    "                    hax0=fig.add_subplot(grid[ib,0])\n",
    "                    #1hax1=fig.add_subplot(212)\n",
    "                    #hax1.set_title('measures ' )\n",
    "                    if ib == 0:\n",
    "                        hax0.set_title('velocs vs measures ' + rgt)\n",
    "\n",
    "                    lats = f[f'/{beam}/latitudes'][()]\n",
    "                    lons = f[f'/{beam}/longitudes'][()]\n",
    "                    coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                    velocs = f[f'/{beam}/velocities'][()]\n",
    "                    v_along=f[f'/{beam}/Measures_v_along'][()]\n",
    "                    xy=np.array(pyproj.proj.Proj(3031)(lons,lats))\n",
    "\n",
    "                    ixs0 = coeffs <= correlation_threshold\n",
    "                    ixs = coeffs > correlation_threshold\n",
    "\n",
    "\n",
    "\n",
    "                    h0 = hax0.scatter(xy[0], velocs, 1, coeffs, vmin = 0, vmax = 1,cmap = 'viridis')\n",
    "                    h1 = hax0.plot(xy[0], v_along, 'k-')\n",
    "\n",
    "                    hax0.set_ylim(-800,800)\n",
    "                    c = plt.colorbar(h0, ax = hax0)\n",
    "                    c.set_label('Correlation coefficient (0 -> 1)')\n",
    "\n",
    "                    h2 = haxMOA.scatter(xy[0][ixs0], xy[1][ixs0], 0.02, 'k')\n",
    "                    h3 = haxMOA.scatter(xy[0][ixs], xy[1][ixs], 0.15, velocs[ixs], vmin = -800, vmax = 800,cmap = 'plasma')\n",
    "\n",
    "\n",
    "\n",
    "        c = plt.colorbar(h3, ax = haxMOA)\n",
    "        c.set_label('Along-track velocity (m/yr)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        outfile = plot_out_location + 'rgt' + rgt + '.' + beam + '_vs_measures_veloc' + str(velocity_number).zfill(2) + '.png'\n",
    "        plt.savefig(outfile, dpi = 200)\n",
    "        if close == True:\n",
    "            plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a list of all available repeat ground tracks in the folder with data in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['0659', '0750', '0848'])\n",
      "['04', '05', '06', '03']\n"
     ]
    }
   ],
   "source": [
    "rgts = {}\n",
    "for filepath in ATL06_files:\n",
    "    filename = filepath.split('/')[-1]\n",
    "    rgt = filename.split('_')[3][0:4]\n",
    "    track = filename.split('_')[3][4:6]\n",
    "#     print(rgt,track)\n",
    "    if not rgt in rgts.keys():\n",
    "        rgts[rgt] = []\n",
    "        rgts[rgt].append(track)\n",
    "    else:\n",
    "        rgts[rgt].append(track)\n",
    "\n",
    "\n",
    "# all rgt values in our study are are in rgts.keys()\n",
    "print(rgts.keys())\n",
    "\n",
    "# available tracks for each rgt are in rgts[rgt]; ex.:\n",
    "print(rgts['0848'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over available rgts and do the correlation processing:\n",
    "\n",
    "Save all results as hdf5 files that can be reloaded and manipulated laver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choices about processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which cycles to process\n",
    "cycles = ['03','04','05','06','07'] # not doing 1 and 2, because don't overlap exactly (this could be future work)\n",
    "\n",
    "### Which beams to process\n",
    "beams = ['gt1l','gt1r','gt2l','gt2r','gt3l','gt3r']\n",
    "\n",
    "### Which product\n",
    "product = 'ATL06'\n",
    "dx = 20 # x_atc coordinate distance, will be different for different products or processed data\n",
    "\n",
    "# Filter preprocessing: \n",
    "smoothing = True # Whether or not to apply a running average filter\n",
    "smoothing_window_size = int(np.round(60 / dx)) # meters / dx [meters]; this is the number of datapoints to smooth over\n",
    "# ex., 60 m smoothing window is a 3 point running average smoothed dataset if dx = 20 for ATL06\n",
    "filt = np.ones(smoothing_window_size) # create the filter to convolve with the data\n",
    "\n",
    "### Control the correlation step:\n",
    "segment_length = 2000 # meters, how wide is the window we are correlating in each step\n",
    "search_width = 1000 # meters, how far in front of and behind the window to check for correlation\n",
    "along_track_step = 100 # meters; how much to jump between each consecutivevelocity determination\n",
    "max_percent_nans = 10 # Maximum % of segment length that can be nans and still do the correlation step\n",
    "\n",
    "#Measures spatial extent and paths to tif files\n",
    "spatial_extent = np.array([-65, -86, -55, -81])\n",
    "\n",
    "#Rodrigo Gomez Fell computer path @ UC\n",
    "measures_Vx_path = '/mnt/user1/Antarctica/Quantarctica3/Glaciology/MEaSUREs Ice Flow Velocity/anta_phase_map_VX.tif'\n",
    "measures_Vy_path = '/mnt/user1/Antarctica/Quantarctica3/Glaciology/MEaSUREs Ice Flow Velocity/anta_phase_map_VY.tif'\n",
    "\n",
    "#jupyter hub path\n",
    "measures_Vx_path = '/srv/shared/surface_velocity/FIS_Velocity/Measures2_FIS_Vx.tif'\n",
    "measures_Vy_path = '/srv/shared/surface_velocity/FIS_Velocity/Measures2_FIS_Vy.tif'\n",
    "\n",
    "# GB local path\n",
    "measures_Vx_path = '/Users/grace/Dropbox/Cornell/projects/003/FIS_data/Measures2_FIS_Vx.tif'\n",
    "measures_Vy_path = '/Users/grace/Dropbox/Cornell/projects/003/FIS_data/Measures2_FIS_Vy.tif'\n",
    "\n",
    "\n",
    "### choices for sub-sample fitting of peak correlation coefficient\n",
    "sub_sample = True\n",
    "max_width = 10 # max # samples wide to fit (half-width)\n",
    "min_width = 4 # min # samples wide to fit (half-width)\n",
    "dx_interp = 0.01 # sub-sample dx (ex., 0.01 interpolates to dx of 0.01 samples or 100 points per sample)\n",
    "plotting = False # if you want to visualize the fit; only use this if you are looking at a single x_atc\n",
    "fit_type = 'cubic' # options: 'raw' (don't sub-sample); 'parabola' (fit parabola; usually poor fit); 'cubic' (fit cubic spline; often overfit)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the correlation processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0659, #0 of 3\n",
      "There are 5 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04,05,06,07\n",
      "rgt 0659 encountered an error in cycle 06 to 07\n",
      "negative dimensions are not allowed\n",
      "\n",
      "Processing rgt 0750, #1 of 3\n",
      "There are 4 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04,05,06\n",
      "\n",
      "Processing rgt 0848, #2 of 3\n",
      "There are 4 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04,05,06\n",
      "rgt 0848 encountered an error in cycle 05 to 06\n",
      "negative dimensions are not allowed\n",
      "Total number of repeat tracks successfully processed = 1\n"
     ]
    }
   ],
   "source": [
    "### CURRENTLY this cell breaks at veloc 2 of 0848; rgt 0659 also breaks; \n",
    "### Working on figuring out why still (GB aug 8,2020)\n",
    "\n",
    "### Create dictionaries to put info in\n",
    "velocities = {}   \n",
    "correlations = {}     \n",
    "lags = {}\n",
    "x_atcs_for_velocities = {}\n",
    "latitudes = {}\n",
    "longitudes = {}\n",
    "#add a v_along variable to test if the problem of the length of measures_v_along is the loop\n",
    "\n",
    "\n",
    "rgts_with_errors = []\n",
    "total_number_repeat_tracks_processed = 0\n",
    "\n",
    "### Loop over each rgt in the data directory\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if ir >= 0: #< len(rgts.keys()):  # this is here in case you want to look at specific rgts\n",
    "        try:\n",
    "            print('\\nProcessing rgt ' + rgt + ', #' +str(ir) + ' of ' + str(len(rgts.keys())))\n",
    "\n",
    "            ### Determine how many files there are for this rgt\n",
    "            rgt_files = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}*_003*.h5'))\n",
    "            n_rgt_files_cycle3_and_after = 0\n",
    "            for file in rgt_files:\n",
    "                if float(file.split('/')[-1].split('_')[3][4:6]) >= 3:\n",
    "                    n_rgt_files_cycle3_and_after += 1\n",
    "\n",
    "            print('There are ' +str(n_rgt_files_cycle3_and_after) + ' files available for this track from cycle 3 onward')\n",
    "\n",
    "\n",
    "            ### Only process if there is at least one repeat track during the time period when data overlapped\n",
    "            if n_rgt_files_cycle3_and_after >= 2:\n",
    "\n",
    "\n",
    "                ### Load necessary data from all available cycles\n",
    "                x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, times, min_seg_ids, segment_ids, cycles_this_rgt, x_ps, y_ps = \\\n",
    "                    load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, datapath, product)\n",
    "                \n",
    "                cycles_this_rgt = sorted(cycles_this_rgt)\n",
    "                \n",
    "                ### Determine # of possible velocities, given how many cycles are available:\n",
    "                n_possible_veloc = len(cycles_this_rgt) -1 # naive, for now; can improve later; We could, for example, do non-consecutive cycles, like 03 and 05\n",
    "                for veloc_number in range(n_possible_veloc):\n",
    "                    \n",
    "                    ### Where to save the results:\n",
    "                    #h5_file_out = f'{out_path}rgt{rgt}_veloc{veloc_number}.hdf5'\n",
    "                    h5_file_out = out_path + 'rgt' + rgt + '_veloc' + str(veloc_number ).zfill(2) + '.hdf5'\n",
    "\n",
    "\n",
    "                    ### Save some metadata\n",
    "                    with h5py.File(h5_file_out,'w') as f:\n",
    "                        f['dx'] = dx \n",
    "                        f['product'] = product \n",
    "                        f['segment_length'] = segment_length \n",
    "                        f['search_width'] = search_width \n",
    "                        f['along_track_step'] = along_track_step \n",
    "                        f['max_percent_nans'] = max_percent_nans \n",
    "                        f['smoothing'] = smoothing \n",
    "                        f['smoothing_window_size'] = smoothing_window_size \n",
    "                        f['process_date'] = str(Time.now().value) \n",
    "                        f['rgt'] = rgt\n",
    "\n",
    "                    ### Which cycles are being processed in the current velocity determination\n",
    "                    cycle1 = cycles_this_rgt[veloc_number]\n",
    "                    cycle2 = cycles_this_rgt[veloc_number+1]\n",
    "                    \n",
    "                    ### Timing of each cycle in the current velocity determination\n",
    "                    t1_string = times[cycle1]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t1 = Time(t1_string)\n",
    "\n",
    "                    t2_string = times[cycle2]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t2 = Time(t2_string)\n",
    "\n",
    "                    ### Elapsed time between cycles\n",
    "                    dt = (t2 - t1).jd # difference in julian days\n",
    "\n",
    "                    ### Create dictionaries\n",
    "                    velocities[rgt] = {}   \n",
    "                    correlations[rgt] = {}     \n",
    "                    lags[rgt] = {}\n",
    "\n",
    "                    ### Loop over each beam\n",
    "                    for beam in beams:\n",
    "\n",
    "                        ### Determine x1s, which are the x_atc coordinates at which each cut out window begins\n",
    "                        # To be common between both repeats, the first point x1 needs to be the larger first value between repeats\n",
    "                        min_x_atc_cycle1 = x_atc[cycle1][beam][0]\n",
    "                        min_x_atc_cycle2 = x_atc[cycle2][beam][0]\n",
    "\n",
    "                        # pick out the track that starts at greater x_atc, and use that as x1s vector\n",
    "                        if min_x_atc_cycle1 != min_x_atc_cycle2: \n",
    "                            x1 = np.nanmax([min_x_atc_cycle1,min_x_atc_cycle2])\n",
    "                            cycle_n = np.arange(0,2)[[min_x_atc_cycle1,min_x_atc_cycle2] == x1][0]\n",
    "                            if cycle_n == 0:\n",
    "                                cycletmp = cycle2\n",
    "                            elif cycle_n == 1:\n",
    "                                cycletmp = cycle1\n",
    "                            #n_segments_this_track = (len(x_atc[cycletmp][beam]) - 2 * search_width/dx) / (along_track_step/dx)\n",
    "\n",
    "                            ### Generate the x1s vector, in the case that the repeat tracks don't start in the same place\n",
    "                            x1s = x_atc[cycletmp][beam][int(search_width/dx)+1:-int(segment_length/dx) - int(search_width/dx):int(along_track_step/dx)]\n",
    "                            #### ! this line updated 2020 07 23\n",
    "                            # x1s = x_atc[cycletmp][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "                            # start at search_width/dx in, so the code never tries to get data outside the edges of this rgt\n",
    "                            # add 1 bc the data are differentiated, and h_li_diff is therefore one point shorter\n",
    "\n",
    "                        elif min_x_atc_cycle1 == min_x_atc_cycle2: # doesn't matter which cycle\n",
    "                            ### Generate the x1s vector, in the case that the repeat tracks do start in the same place\n",
    "                            x1s = x_atc[cycle1][beam][int(search_width/dx)+1:-int(segment_length/dx) - int(search_width/dx):int(along_track_step/dx)]\n",
    "                            #### ! this line updated 2020 07 23\n",
    "                            #x1s = x_atc[cycle1][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "\n",
    "\n",
    "\n",
    "                        ### Determine xend, where the x1s vector ends: smaller value for both beams, if different\n",
    "                        max_x_atc_cycle1 = x_atc[cycle1][beam][-1]- search_width/dx\n",
    "                        max_x_atc_cycle2 = x_atc[cycle2][beam][-1]- search_width/dx\n",
    "                        smallest_xatc = np.min([max_x_atc_cycle1,max_x_atc_cycle2])\n",
    "                        ixmax = np.where(x1s >= (smallest_xatc - search_width/dx))\n",
    "                        if len(ixmax[0]) >= 1:\n",
    "                            ixtmp = ixmax[0][0]\n",
    "                            x1s = x1s[:ixtmp]\n",
    "\n",
    "                        ### Create vectors to store results in\n",
    "                        velocities[rgt][beam] = np.empty_like(x1s)\n",
    "                        correlations[rgt][beam] = np.empty_like(x1s)\n",
    "                        lags[rgt][beam] = np.empty_like(x1s)\n",
    "\n",
    "                        midpoints_x_atc = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lat = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lon = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_seg_ids = np.empty(np.shape(x1s)) # for writing out \n",
    "                                                                                \n",
    "                        ### Entire x_atc vectors for both cycles    \n",
    "                        x_full_t1 = x_atc[cycle1][beam]\n",
    "                        x_full_t2 = x_atc[cycle2][beam]\n",
    "\n",
    "                        ### Loop over x1s, positions along track that each window starts at\n",
    "                        for xi, x1 in enumerate(x1s):\n",
    "                            \n",
    "                            ### Cut out data: small chunk of data at time t1 (first cycle)\n",
    "                            ix_x1 = np.arange(len(x_full_t1))[x_full_t1 >= x1][0] # Index of first point that is greater than x1\n",
    "                            ix_x2 = ix_x1 + int(np.round(segment_length/dx)) # ix_x1 + number of datapoints within the desired segment length\n",
    "                            x_t1 = x_full_t1[ix_x1:ix_x2] # cut out x_atc values, first cycle\n",
    "                            lats_t1 = lats[cycle1][beam][ix_x1:ix_x2] # cut out latitude values, first cycle\n",
    "                            lons_t1 = lons[cycle1][beam][ix_x1:ix_x2] # cut out longitude values, first cycle\n",
    "                            seg_ids_t1 = segment_ids[cycle1][beam][ix_x1:ix_x2] # cut out segment_ids, first cycle\n",
    "                            h_li1 = h_li_diff[cycle1][beam][ix_x1-1:ix_x2-1] # cut out land ice height values, first cycle; start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # Find segment midpoints; this is the position where we will assign the velocity measurement from each window\n",
    "                            n = len(x_t1)\n",
    "                            midpt_ix = int(np.floor(n/2))\n",
    "                            midpoints_x_atc[xi] = x_t1[midpt_ix]\n",
    "                            midpoints_lat[xi] = lats_t1[midpt_ix]\n",
    "                            midpoints_lon[xi] = lons_t1[midpt_ix]\n",
    "                            midpoints_seg_ids[xi] = seg_ids_t1[midpt_ix]\n",
    "                            \n",
    "                            ### Cut out data: wider chunk of data at time t2 (second cycle)\n",
    "                            ix_x3 = ix_x1 - int(np.round(search_width/dx)) # extend on earlier end by number of indices in search_width\n",
    "                            ix_x4 = ix_x2 + int(np.round(search_width/dx)) # extend on later end by number of indices in search_width\n",
    "                            x_t2 = x_full_t2[ix_x3:ix_x4] # cut out x_atc values, second cycle\n",
    "                            h_li2 = h_li_diff[cycle2][beam][ix_x3-1:ix_x4-1]# cut out land ice height values, second cycle; start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            ### Determine number of nans in each data chunk\n",
    "                            n_nans1 = np.sum(np.isnan(h_li_raw[cycle1][beam][ix_x1:ix_x2]))\n",
    "                            n_nans2 = np.sum(np.isnan(h_li_raw[cycle2][beam][ix_x3:ix_x4]))\n",
    "                            \n",
    "                            ### Only process if there are fewer than 10% nans in either data chunk:\n",
    "                            if (n_nans1 / len(h_li1) <= max_percent_nans/100) and (n_nans2 / len(h_li2) <= max_percent_nans/100):\n",
    "\n",
    "                                # Detrend both chunks of data\n",
    "                                h_li1 = detrend(h_li1,type = 'linear')\n",
    "                                h_li2 = detrend(h_li2,type = 'linear')\n",
    "\n",
    "                                # Normalize both chunks of data, if desired\n",
    "                                # h_li1 = h_li1 / np.nanmax(np.abs(h_li1))\n",
    "                                # h_li2 = h_li2 / np.nanmax(np.abs(h_li2))\n",
    "\n",
    "                                ### Correlate the old and new data\n",
    "                                # We made the second data vector longer than the first, so the valid method returns values\n",
    "                                corr = correlate(h_li1, h_li2, mode = 'valid', method = 'direct') \n",
    "\n",
    "                                ### Normalize correlation function by autocorrelations\n",
    "                                # Normalizing coefficient changes with each step along track; this section determines a changing along track normalizing coefficiant\n",
    "                                coeff_a_val = np.sum(h_li1**2)\n",
    "                                coeff_b_val = np.zeros(len(h_li2) - len(h_li1)+1)\n",
    "                                for shift in range(len(h_li2) - len(h_li1)+1):\n",
    "                                    h_li2_section = h_li2[shift:shift + len(h_li1)]\n",
    "                                    coeff_b_val[shift] = np.sum(h_li2_section **2)\n",
    "                                norm_vec = np.sqrt(coeff_a_val * coeff_b_val)\n",
    "                                corr_normed = corr / np.flip(norm_vec) # i don't really understand why this has to flip, but otherwise it yields correlation values above 1...\n",
    "\n",
    "                                ### Create a vector of lags for the correlation function\n",
    "                                lagvec = np.arange(- int(np.round(search_width/dx)), int(search_width/dx) +1,1)# for mode = 'valid'\n",
    "\n",
    "                                ### Convert lag to distance\n",
    "                                shift_vec = lagvec * dx\n",
    "\n",
    "                                ### ID peak correlation coefficient\n",
    "                                ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "                                \n",
    "                                ### Save correlation coefficient, best lag, velocity, etc at the location of peak correlation coefficient\n",
    "                                best_lag = lagvec[ix_peak]\n",
    "                                best_shift = shift_vec[ix_peak]\n",
    "\n",
    "#                                 ### ID peak correlation coefficient; 'raw' = no sub-sampling\n",
    "#                                 plotting = False\n",
    "#                                 best_lag, peak_corr_value = find_correlation_peak(lagvec, shift_vec, corr_normed, max_width, min_width, dx_interp, type, plotting)\n",
    "#                                 best_shift = best_lag * dx\n",
    "\n",
    "                                velocities[rgt][beam][xi] = best_shift/(dt/365)\n",
    "                                correlations[rgt][beam][xi] = corr_normed[ix_peak]\n",
    "                                lags[rgt][beam][xi] = lagvec[ix_peak]\n",
    "                            else:\n",
    "                                ### If there are too many nans, just save a nan\n",
    "                                velocities[rgt][beam][xi] = np.nan\n",
    "                                correlations[rgt][beam][xi] = np.nan\n",
    "                                lags[rgt][beam][xi] = np.nan\n",
    "                                \n",
    "                         \n",
    "                        xy=np.array(pyproj.Proj(3031)(midpoints_lon, midpoints_lat))    \n",
    "                        ### Add velocities to hdf5 file for each beam\n",
    "                        with h5py.File(h5_file_out, 'a') as f:\n",
    "                            f[beam +'/x_atc'] = midpoints_x_atc # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/latitudes'] = midpoints_lat # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/longitudes'] = midpoints_lon # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/velocities'] = velocities[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/correlation_coefficients'] = correlations[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/best_lags'] = lags[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/segment_ids'] = midpoints_seg_ids\n",
    "                            f[beam +'/first_cycle_time'] = str(Time(times[cycle1][beam][0]))\n",
    "                            f[beam +'/second_cycle_time'] = str(Time(times[cycle2][beam][0]))\n",
    "                            f[beam +'/Measures_v_along'] = get_measures_along_track_velocity(xy[0], xy[1] , spatial_extent, measures_Vx_path, measures_Vy_path)\n",
    "\n",
    "\n",
    "                    ### Record which cycles contributed to these results\n",
    "                    with h5py.File(h5_file_out, 'a') as f:\n",
    "                        f['contributing_cycles'] = ','.join([cycle1,cycle2])\n",
    "\n",
    "            \n",
    "                total_number_repeat_tracks_processed += 1\n",
    "                \n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f'rgt {rgt} encountered an error in cycle {cycle1} to {cycle2}')\n",
    "            print(e)\n",
    "            rgts_with_errors.append(rgt)\n",
    "            \n",
    "print(f'Total number of repeat tracks successfully processed = {total_number_repeat_tracks_processed}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load data, make a map of correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/                        Group\r\n",
      "/along_track_step        Dataset {SCALAR}\r\n",
      "/contributing_cycles     Dataset {SCALAR}\r\n",
      "/dx                      Dataset {SCALAR}\r\n",
      "/gt1l                    Group\r\n",
      "/gt1l/Measures_v_along   Dataset {4376}\r\n",
      "/gt1l/best_lags          Dataset {4376}\r\n",
      "/gt1l/correlation_coefficients Dataset {4376}\r\n",
      "/gt1l/first_cycle_time   Dataset {SCALAR}\r\n",
      "/gt1l/latitudes          Dataset {4376}\r\n",
      "/gt1l/longitudes         Dataset {4376}\r\n",
      "/gt1l/second_cycle_time  Dataset {SCALAR}\r\n",
      "/gt1l/segment_ids        Dataset {4376}\r\n",
      "/gt1l/velocities         Dataset {4376}\r\n",
      "/gt1l/x_atc              Dataset {4376}\r\n",
      "/gt1r                    Group\r\n",
      "/gt1r/Measures_v_along   Dataset {4374}\r\n",
      "/gt1r/best_lags          Dataset {4374}\r\n",
      "/gt1r/correlation_coefficients Dataset {4374}\r\n",
      "/gt1r/first_cycle_time   Dataset {SCALAR}\r\n",
      "/gt1r/latitudes          Dataset {4374}\r\n",
      "/gt1r/longitudes         Dataset {4374}\r\n",
      "/gt1r/second_cycle_time  Dataset {SCALAR}\r\n",
      "/gt1r/segment_ids        Dataset {4374}\r\n",
      "/gt1r/velocities         Dataset {4374}\r\n",
      "/gt1r/x_atc              Dataset {4374}\r\n",
      "/gt2l                    Group\r\n",
      "/gt2l/Measures_v_along   Dataset {4286}\r\n",
      "/gt2l/best_lags          Dataset {4286}\r\n",
      "/gt2l/correlation_coefficients Dataset {4286}\r\n",
      "/gt2l/first_cycle_time   Dataset {SCALAR}\r\n",
      "/gt2l/latitudes          Dataset {4286}\r\n",
      "/gt2l/longitudes         Dataset {4286}\r\n",
      "/gt2l/second_cycle_time  Dataset {SCALAR}\r\n",
      "/gt2l/segment_ids        Dataset {4286}\r\n",
      "/gt2l/velocities         Dataset {4286}\r\n",
      "/gt2l/x_atc              Dataset {4286}\r\n",
      "/gt2r                    Group\r\n",
      "/gt2r/Measures_v_along   Dataset {4283}\r\n",
      "/gt2r/best_lags          Dataset {4283}\r\n",
      "/gt2r/correlation_coefficients Dataset {4283}\r\n",
      "/gt2r/first_cycle_time   Dataset {SCALAR}\r\n",
      "/gt2r/latitudes          Dataset {4283}\r\n",
      "/gt2r/longitudes         Dataset {4283}\r\n",
      "/gt2r/second_cycle_time  Dataset {SCALAR}\r\n",
      "/gt2r/segment_ids        Dataset {4283}\r\n",
      "/gt2r/velocities         Dataset {4283}\r\n",
      "/gt2r/x_atc              Dataset {4283}\r\n",
      "/gt3l                    Group\r\n",
      "/gt3l/Measures_v_along   Dataset {4195}\r\n",
      "/gt3l/best_lags          Dataset {4195}\r\n",
      "/gt3l/correlation_coefficients Dataset {4195}\r\n",
      "/gt3l/first_cycle_time   Dataset {SCALAR}\r\n",
      "/gt3l/latitudes          Dataset {4195}\r\n",
      "/gt3l/longitudes         Dataset {4195}\r\n",
      "/gt3l/second_cycle_time  Dataset {SCALAR}\r\n",
      "/gt3l/segment_ids        Dataset {4195}\r\n",
      "/gt3l/velocities         Dataset {4195}\r\n",
      "/gt3l/x_atc              Dataset {4195}\r\n",
      "/gt3r                    Group\r\n",
      "/gt3r/Measures_v_along   Dataset {4192}\r\n",
      "/gt3r/best_lags          Dataset {4192}\r\n",
      "/gt3r/correlation_coefficients Dataset {4192}\r\n",
      "/gt3r/first_cycle_time   Dataset {SCALAR}\r\n",
      "/gt3r/latitudes          Dataset {4192}\r\n",
      "/gt3r/longitudes         Dataset {4192}\r\n",
      "/gt3r/second_cycle_time  Dataset {SCALAR}\r\n",
      "/gt3r/segment_ids        Dataset {4192}\r\n",
      "/gt3r/velocities         Dataset {4192}\r\n",
      "/gt3r/x_atc              Dataset {4192}\r\n",
      "/max_percent_nans        Dataset {SCALAR}\r\n",
      "/process_date            Dataset {SCALAR}\r\n",
      "/product                 Dataset {SCALAR}\r\n",
      "/rgt                     Dataset {SCALAR}\r\n",
      "/search_width            Dataset {SCALAR}\r\n",
      "/segment_length          Dataset {SCALAR}\r\n",
      "/smoothing               Dataset {SCALAR}\r\n",
      "/smoothing_window_size   Dataset {SCALAR}\r\n"
     ]
    }
   ],
   "source": [
    "### What's in each results file\n",
    "\n",
    "!h5ls -r /Users/grace/Dropbox/Cornell/projects/003/ATL06_0848/out/rgt0848_veloc0.hdf5\n",
    "# !h5ls -r /media/rag110/ADATA\\ SD700/ICESat2/output/FIS/rgt0004_veloc0.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot measures vs our velocity product; miscellaneous track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea5d53cdd6a4dd39fa671123c7f0e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cmap': 'gray', 'clim': [14000, 17000], 'extent': array([-887950., -356950.,  183825.,  561825.]), 'origin': 'lower'}\n"
     ]
    }
   ],
   "source": [
    "# rgt = '0848'\n",
    "plot_out_location = out_path\n",
    "correlation_threshold = 0.65\n",
    "velocity_number = 1\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if rgt == '0848':\n",
    "        plot_measures_along_track_comparison(rgt, out_path, correlation_threshold, plot_out_location, map_data_root, velocity_number, close = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daniel - you can ignore everything below here; these are just cells I was using to debug stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the peak finder script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "            ix_next_trough = np.where(corr_diff[ix_peak-1:] > 0)[0][0] + ix_peak # -1 b/c diff\n",
    "            ix_prev_trough = ix_peak - np.where(np.flip(corr_diff[:ix_peak-1]) < 0)[0][0]\n",
    "            # if width is greater than min, pick smaller width, so is same on both sides\n",
    "            width = np.min([ix_peak - ix_prev_trough, -(ix_peak - ix_next_trough)])\n",
    "#             if width > max_width:\n",
    "#                 width = max_width\n",
    "#             elif width < min_width:\n",
    "#                 width = min_width\n",
    "#             left_width = width\n",
    "#             right_width = width\n",
    "            print(ix_prev_trough)\n",
    "            print(ix_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74042950215b4cd19db63e59d36fa8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    min_width = 2\n",
    "    plotting = True\n",
    "    # def find_correlation_peak(lagvec, shift_vec, corr_normed, max_width, min_width, dx_interp, fit_type, plotting):\n",
    "    \n",
    "    # max_width = how many SAMPLES wide the half peak can be, max, samples\n",
    "    # min_width = the narrowest the half peak can be, samples\n",
    "    # dx_interp = interpolated sample spacing, in units of SAMPLES\n",
    "    \n",
    "    from scipy.interpolate import interp1d\n",
    "    \n",
    "    # peak of un-interpolated data\n",
    "    ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "    ix_peak = np.where(corr_normed == np.nanmax(corr_normed))[0][0]\n",
    "    \n",
    "    if fit_type == 'raw':\n",
    "        best_lag = lagvec[ix_peak]\n",
    "        best_shift = shift_vec[ix_peak]\n",
    "        peak_corr_value = corr_normed[ix_peak]\n",
    "        \n",
    "    else:\n",
    "        ### Assume we're doing a sub-sample fit\n",
    "        # find troughs before and after peak\n",
    "        \n",
    "        corr_diff = corr_normed[1:] - corr_normed[:-1] # to find troughs, zero-crossings\n",
    "\n",
    "        # cases:\n",
    "        # ix_peak is too close to the start (ix_peak < min_width)\n",
    "            # calculate right width by finding trough or max/min, use ix_peak as left width\n",
    "        if ix_peak < min_width:\n",
    "            left_width = ix_peak # go to left edge\n",
    "            # determine right width\n",
    "            if ix_peak == 0:\n",
    "                ix_next_trough = np.where(corr_diff[ix_peak:] > 0)[0][0] + ix_peak\n",
    "            else:\n",
    "                ix_next_trough = np.where(corr_diff[ix_peak-1:] > 0)[0][0] + ix_peak\n",
    "            width_next_trough = -(ix_peak - ix_next_trough)\n",
    "            if width_next_trough < min_width:\n",
    "                right_width = min_width\n",
    "            elif width_next_trough > max_width:\n",
    "                right_width = max_width\n",
    "            else:\n",
    "                right_width = width_next_trough\n",
    "\n",
    "        # ix_peak is too close to the end (len(corr_normed) - ix_peak < min_width)\n",
    "            # calculate left width by finding trough or max/min, use len(corr_normed) - ix_peak as right width\n",
    "        elif len(corr_normed) - ix_peak < min_width:\n",
    "            right_width = len(corr_normed) - ix_peak -1\n",
    "            #determine left width\n",
    "            ix_prev_trough = ix_peak - np.where(np.flip(corr_diff[:ix_peak-1]) < 0)[0][0]\n",
    "            width_prev_trough = ix_peak - ix_prev_trough\n",
    "            if width_prev_trough < min_width:\n",
    "                left_width = min_width\n",
    "            elif width_prev_trough > max_width:\n",
    "                left_width = max_width\n",
    "            else:\n",
    "                left_width = width_prev_trough\n",
    "\n",
    "        # otherwise\n",
    "        else:\n",
    "            # calculate both left and right width\n",
    "            ix_next_trough = np.where(corr_diff[ix_peak-1:] > 0)[0][0] + ix_peak -1\n",
    "            ix_prev_trough = ix_peak - np.where(np.flip(corr_diff[:ix_peak-1]) < 0)[0][0] -1\n",
    "            # if width is greater than min, pick smaller width, so is same on both sides\n",
    "            width = np.min([ix_peak - ix_prev_trough, -(ix_peak - ix_next_trough)])\n",
    "            if width > max_width:\n",
    "                width = max_width\n",
    "            elif width < min_width:\n",
    "                width = min_width\n",
    "            left_width = width\n",
    "            right_width = width\n",
    "            \n",
    "#         # deal with edges; just go to edge\n",
    "#         if ix_peak + width >= len(corr_normed):\n",
    "#             right_width = len(corr_normed) - ix_peak -1\n",
    "#         else:\n",
    "#             right_width = width\n",
    "            \n",
    "#         if width>= ix_peak:\n",
    "#             left_width = ix_peak\n",
    "#         else:\n",
    "#             left_width = width\n",
    "        \n",
    "        # cut out data and an x vector before and after the peak (for plotting)\n",
    "        dcut = corr_normed[ix_peak - left_width: ix_peak + right_width +1]\n",
    "        xvec_cut = np.arange(ix_peak - left_width,  ix_peak + right_width + 1)\n",
    "        lagvec_cut = lagvec[xvec_cut]\n",
    "        \n",
    "        if fit_type == 'parabola':        \n",
    "            # fit parabola; in these data, usually a poor fit\n",
    "            fit_coeffs = np.polyfit(xvec_cut, dcut, deg=2, full = True)\n",
    "\n",
    "            # create xvec to interpolate the parabola onto and compute \n",
    "            interp_xvec = np.arange(ix_peak - left_width,  ix_peak + right_width + 1, 0.01)\n",
    "            fit = np.polyval(p = fit_coeffs[0], x = interp_xvec)\n",
    "\n",
    "            # interp lagvec\n",
    "            lagvec_interp = np.arange(lagvec[ix_peak - left_width], lagvec[ix_peak + right_width + 1], dx_interp)\n",
    "\n",
    "            # compute peak\n",
    "            ix_peak_interp = np.where(fit == np.max(fit))[0][0]\n",
    "            best_lag = interp_xvec[ix_peak_interp]\n",
    "            peak_corr_value = fit[ix_peak_interp]\n",
    "\n",
    "        elif fit_type == 'cubic':\n",
    "#             # find troughs before and after peak\n",
    "#             corr_diff = corr_normed[1:] - corr_normed[:-1]\n",
    "#             ix_next_trough = np.where(corr_diff[ix_peak:] > 0)[0][0] + ix_peak\n",
    "#             ix_prev_trough = ix_peak - np.where(np.flip(corr_diff[:ix_peak]) < 0)[0][0]\n",
    "#             width = np.min([ix_peak - ix_prev_trough, -(ix_peak - ix_next_trough)])\n",
    "#             if width > max_width:\n",
    "#                 width = max_width\n",
    "#             elif width < min_width:\n",
    "#                 width = min_width\n",
    "\n",
    "#             # cut out data and an x vector before and after the peak (for plotting)\n",
    "#             dcut = corr_normed[ix_peak - width: ix_peak + width +1]\n",
    "#             xvec_cut = np.arange(ix_peak - width,  ix_peak + width + 1)\n",
    "#             lagvec_cut = lagvec[xvec_cut]\n",
    "\n",
    "#             if np.sum(np.isnan(dcut)) >0:\n",
    "#                 print(str(np.sum(np.isnan(dcut))) + ' nans')\n",
    "\n",
    "            # cubic spline interpolation\n",
    "            # create xvector to interpolate onto\n",
    "            interp_xvec = np.arange(ix_peak - left_width,  ix_peak + right_width , dx_interp)\n",
    "            # create interpolation function\n",
    "            f = interp1d(xvec_cut, dcut, kind = 'cubic')\n",
    "            # evaluate interpolation function on new xvector\n",
    "            fit = f(interp_xvec)\n",
    "\n",
    "            # interpolate lagvec\n",
    "            lagvec_interp = np.arange(lagvec[ix_peak - left_width], lagvec[ix_peak + right_width], dx_interp)\n",
    "\n",
    "            # compute peak\n",
    "            ix_peak_interp = np.where(fit == np.max(fit))[0][0]\n",
    "            best_lag = lagvec_interp[ix_peak_interp]\n",
    "            peak_corr_value = fit[ix_peak_interp]\n",
    "\n",
    "    if plotting:\n",
    "        plt.figure()\n",
    "        plt.plot(lagvec, corr_normed, 'k-')\n",
    "        plt.xlabel('Lag (samples)')\n",
    "        plt.ylabel('Correlation coefficient')\n",
    "        plt.plot(lagvec[ix_peak], corr_normed[ix_peak],'b*')\n",
    "        plt.text(lagvec[-20], np.max(corr_normed), 'best raw lag: ' + str(lagvec[ix_peak]) + '\\ncorr coeff: ' + str(corr_normed[ix_peak]))\n",
    "\n",
    "\n",
    "        if fit_type != 'raw':\n",
    "            plt.plot(lagvec_cut, dcut, 'b*')\n",
    "            plt.plot(lagvec_cut, dcut, 'b-')\n",
    "            plt.plot(lagvec_interp, fit, 'r--')\n",
    "            plt.plot(lagvec_interp[ix_peak_interp], fit[ix_peak_interp], 'r.')\n",
    "            plt.text(lagvec[-20], 0.7*np.max(corr_normed), 'best interpolated lag: ' + str(best_lag) + '\\ncorr coeff: ' + str(peak_corr_value))\n",
    "\n",
    "    \n",
    "    ###return best_lag, peak_corr_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing rgt 0848, #0 of 1\n",
      "There are 2 files available for this track from cycle 3 onward\n",
      "Cycles available: 03,04\n",
      "Total number of repeat tracks successfully processed = 1\n"
     ]
    }
   ],
   "source": [
    "##### This cell just stops at a point when there is a good xcorr function, to debug what's above\n",
    "\n",
    "### Create dictionaries to put info in\n",
    "velocities = {}   \n",
    "correlations = {}     \n",
    "lags = {}\n",
    "x_atcs_for_velocities = {}\n",
    "latitudes = {}\n",
    "longitudes = {}\n",
    "#add a v_along variable to test if the problem of the length of measures_v_along is the loop\n",
    "\n",
    "\n",
    "rgts_with_errors = []\n",
    "total_number_repeat_tracks_processed = 0\n",
    "\n",
    "### Loop over each rgt in the data directory\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if ir >= 0: #< len(rgts.keys()):  # this is here in case you want to look at specific rgts\n",
    "        try:\n",
    "            print('\\nProcessing rgt ' + rgt + ', #' +str(ir) + ' of ' + str(len(rgts.keys())))\n",
    "\n",
    "            ### Determine how many files there are for this rgt\n",
    "            rgt_files = glob.glob(os.path.join(datapath, f'*ATL06_*_{rgt}*_003*.h5'))\n",
    "            n_rgt_files_cycle3_and_after = 0\n",
    "            for file in rgt_files:\n",
    "                if float(file.split('/')[-1].split('_')[3][4:6]) >= 3:\n",
    "                    n_rgt_files_cycle3_and_after += 1\n",
    "\n",
    "            print('There are ' +str(n_rgt_files_cycle3_and_after) + ' files available for this track from cycle 3 onward')\n",
    "\n",
    "\n",
    "            ### Only process if there is at least one repeat track during the time period when data overlapped\n",
    "            if n_rgt_files_cycle3_and_after >= 2:\n",
    "\n",
    "\n",
    "                ### Load necessary data from all available cycles\n",
    "                x_atc, lats, lons, h_li_raw, h_li_raw_NoNans, h_li, h_li_diff, times, min_seg_ids, segment_ids, cycles_this_rgt, x_ps, y_ps = \\\n",
    "                    load_data_by_rgt(rgt, smoothing, smoothing_window_size, dx, datapath, product)\n",
    "                \n",
    "                cycles_this_rgt = sorted(cycles_this_rgt)\n",
    "                \n",
    "                ### Determine # of possible velocities, given how many cycles are available:\n",
    "                n_possible_veloc = len(cycles_this_rgt) -1 # naive, for now; can improve later; We could, for example, do non-consecutive cycles, like 03 and 05\n",
    "                for veloc_number in range(n_possible_veloc):\n",
    "                    \n",
    "                    ### Where to save the results:\n",
    "                    h5_file_out = f'{out_path}rgt{rgt}_veloc{veloc_number}.hdf5'\n",
    "                    \n",
    "                    ### Save some metadata\n",
    "                    with h5py.File(h5_file_out,'w') as f:\n",
    "                        f['dx'] = dx \n",
    "                        f['product'] = product \n",
    "                        f['segment_length'] = segment_length \n",
    "                        f['search_width'] = search_width \n",
    "                        f['along_track_step'] = along_track_step \n",
    "                        f['max_percent_nans'] = max_percent_nans \n",
    "                        f['smoothing'] = smoothing \n",
    "                        f['smoothing_window_size'] = smoothing_window_size \n",
    "                        f['process_date'] = str(Time.now().value) \n",
    "                        f['rgt'] = rgt\n",
    "\n",
    "                    ### Which cycles are being processed in the current velocity determination\n",
    "                    cycle1 = cycles_this_rgt[veloc_number]\n",
    "                    cycle2 = cycles_this_rgt[veloc_number+1]\n",
    "                    \n",
    "                    ### Timing of each cycle in the current velocity determination\n",
    "                    t1_string = times[cycle1]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t1 = Time(t1_string)\n",
    "\n",
    "                    t2_string = times[cycle2]['gt1l'][0].astype(str) #figure out later if just picking hte first one it ok\n",
    "                    t2 = Time(t2_string)\n",
    "\n",
    "                    ### Elapsed time between cycles\n",
    "                    dt = (t2 - t1).jd # difference in julian days\n",
    "\n",
    "                    ### Create dictionaries\n",
    "                    velocities[rgt] = {}   \n",
    "                    correlations[rgt] = {}     \n",
    "                    lags[rgt] = {}\n",
    "\n",
    "                    ### Loop over each beam\n",
    "                    for beam in beams:\n",
    "\n",
    "                        ### Determine x1s, which are the x_atc coordinates at which each cut out window begins\n",
    "                        # To be common between both repeats, the first point x1 needs to be the larger first value between repeats\n",
    "                        min_x_atc_cycle1 = x_atc[cycle1][beam][0]\n",
    "                        min_x_atc_cycle2 = x_atc[cycle2][beam][0]\n",
    "\n",
    "                        # pick out the track that starts at greater x_atc, and use that as x1s vector\n",
    "                        if min_x_atc_cycle1 != min_x_atc_cycle2: \n",
    "                            x1 = np.nanmax([min_x_atc_cycle1,min_x_atc_cycle2])\n",
    "                            cycle_n = np.arange(0,2)[[min_x_atc_cycle1,min_x_atc_cycle2] == x1][0]\n",
    "                            if cycle_n == 0:\n",
    "                                cycletmp = cycle2\n",
    "                            elif cycle_n == 1:\n",
    "                                cycletmp = cycle1\n",
    "                            #n_segments_this_track = (len(x_atc[cycletmp][beam]) - 2 * search_width/dx) / (along_track_step/dx)\n",
    "\n",
    "                            ### Generate the x1s vector, in the case that the repeat tracks don't start in the same place\n",
    "                            x1s = x_atc[cycletmp][beam][int(search_width/dx)+1:-int(segment_length/dx) - int(search_width/dx):int(along_track_step/dx)]\n",
    "                            #### ! this line updated 2020 07 23\n",
    "                            # x1s = x_atc[cycletmp][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "                            # start at search_width/dx in, so the code never tries to get data outside the edges of this rgt\n",
    "                            # add 1 bc the data are differentiated, and h_li_diff is therefore one point shorter\n",
    "\n",
    "                        elif min_x_atc_cycle1 == min_x_atc_cycle2: # doesn't matter which cycle\n",
    "                            ### Generate the x1s vector, in the case that the repeat tracks do start in the same place\n",
    "                            x1s = x_atc[cycle1][beam][int(search_width/dx)+1:-int(segment_length/dx) - int(search_width/dx):int(along_track_step/dx)]\n",
    "                            #### ! this line updated 2020 07 23\n",
    "                            #x1s = x_atc[cycle1][beam][int(search_width/dx)+1::int(search_width/dx)]\n",
    "\n",
    "\n",
    "\n",
    "                        ### Determine xend, where the x1s vector ends: smaller value for both beams, if different\n",
    "                        max_x_atc_cycle1 = x_atc[cycle1][beam][-1]- search_width/dx\n",
    "                        max_x_atc_cycle2 = x_atc[cycle2][beam][-1]- search_width/dx\n",
    "                        smallest_xatc = np.min([max_x_atc_cycle1,max_x_atc_cycle2])\n",
    "                        ixmax = np.where(x1s >= (smallest_xatc - search_width/dx))\n",
    "                        if len(ixmax[0]) >= 1:\n",
    "                            ixtmp = ixmax[0][0]\n",
    "                            x1s = x1s[:ixtmp]\n",
    "\n",
    "                        ### Create vectors to store results in\n",
    "                        velocities[rgt][beam] = np.empty_like(x1s)\n",
    "                        correlations[rgt][beam] = np.empty_like(x1s)\n",
    "                        lags[rgt][beam] = np.empty_like(x1s)\n",
    "\n",
    "                        midpoints_x_atc = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lat = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_lon = np.empty(np.shape(x1s)) # for writing out \n",
    "                        midpoints_seg_ids = np.empty(np.shape(x1s)) # for writing out \n",
    "                                                                                \n",
    "                        ### Entire x_atc vectors for both cycles    \n",
    "                        x_full_t1 = x_atc[cycle1][beam]\n",
    "                        x_full_t2 = x_atc[cycle2][beam]\n",
    "\n",
    "                        ### Loop over x1s, positions along track that each window starts at\n",
    "                        for xi, x1 in enumerate(x1s[199:200]):  # 0:100 for one that just has a nice peak in the middle\n",
    "                            # 199:200 - test trough logic\n",
    "                            \n",
    "                            ### Cut out data: small chunk of data at time t1 (first cycle)\n",
    "                            ix_x1 = np.arange(len(x_full_t1))[x_full_t1 >= x1][0] # Index of first point that is greater than x1\n",
    "                            ix_x2 = ix_x1 + int(np.round(segment_length/dx)) # ix_x1 + number of datapoints within the desired segment length\n",
    "                            x_t1 = x_full_t1[ix_x1:ix_x2] # cut out x_atc values, first cycle\n",
    "                            lats_t1 = lats[cycle1][beam][ix_x1:ix_x2] # cut out latitude values, first cycle\n",
    "                            lons_t1 = lons[cycle1][beam][ix_x1:ix_x2] # cut out longitude values, first cycle\n",
    "                            seg_ids_t1 = segment_ids[cycle1][beam][ix_x1:ix_x2] # cut out segment_ids, first cycle\n",
    "                            h_li1 = h_li_diff[cycle1][beam][ix_x1-1:ix_x2-1] # cut out land ice height values, first cycle; start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            # Find segment midpoints; this is the position where we will assign the velocity measurement from each window\n",
    "                            n = len(x_t1)\n",
    "                            midpt_ix = int(np.floor(n/2))\n",
    "                            midpoints_x_atc[xi] = x_t1[midpt_ix]\n",
    "                            midpoints_lat[xi] = lats_t1[midpt_ix]\n",
    "                            midpoints_lon[xi] = lons_t1[midpt_ix]\n",
    "                            midpoints_seg_ids[xi] = seg_ids_t1[midpt_ix]\n",
    "                            \n",
    "                            ### Cut out data: wider chunk of data at time t2 (second cycle)\n",
    "                            ix_x3 = ix_x1 - int(np.round(search_width/dx)) # extend on earlier end by number of indices in search_width\n",
    "                            ix_x4 = ix_x2 + int(np.round(search_width/dx)) # extend on later end by number of indices in search_width\n",
    "                            x_t2 = x_full_t2[ix_x3:ix_x4] # cut out x_atc values, second cycle\n",
    "                            h_li2 = h_li_diff[cycle2][beam][ix_x3-1:ix_x4-1]# cut out land ice height values, second cycle; start 1 index earlier because \n",
    "                            # the h_li_diff data are differentiated, and therefore one sample shorter\n",
    "\n",
    "                            ### Determine number of nans in each data chunk\n",
    "                            n_nans1 = np.sum(np.isnan(h_li_raw[cycle1][beam][ix_x1:ix_x2]))\n",
    "                            n_nans2 = np.sum(np.isnan(h_li_raw[cycle2][beam][ix_x3:ix_x4]))\n",
    "                            \n",
    "                            ### Only process if there are fewer than 10% nans in either data chunk:\n",
    "                            if (n_nans1 / len(h_li1) <= max_percent_nans/100) and (n_nans2 / len(h_li2) <= max_percent_nans/100):\n",
    "\n",
    "                                # Detrend both chunks of data\n",
    "                                h_li1 = detrend(h_li1,type = 'linear')\n",
    "                                h_li2 = detrend(h_li2,type = 'linear')\n",
    "\n",
    "                                # Normalize both chunks of data, if desired\n",
    "                                # h_li1 = h_li1 / np.nanmax(np.abs(h_li1))\n",
    "                                # h_li2 = h_li2 / np.nanmax(np.abs(h_li2))\n",
    "\n",
    "                                ### Correlate the old and new data\n",
    "                                # We made the second data vector longer than the first, so the valid method returns values\n",
    "                                corr = correlate(h_li1, h_li2, mode = 'valid', method = 'direct') \n",
    "\n",
    "                                ### Normalize correlation function by autocorrelations\n",
    "                                # Normalizing coefficient changes with each step along track; this section determines a changing along track normalizing coefficiant\n",
    "                                coeff_a_val = np.sum(h_li1**2)\n",
    "                                coeff_b_val = np.zeros(len(h_li2) - len(h_li1)+1)\n",
    "                                for shift in range(len(h_li2) - len(h_li1)+1):\n",
    "                                    h_li2_section = h_li2[shift:shift + len(h_li1)]\n",
    "                                    coeff_b_val[shift] = np.sum(h_li2_section **2)\n",
    "                                norm_vec = np.sqrt(coeff_a_val * coeff_b_val)\n",
    "                                corr_normed = corr / np.flip(norm_vec) # i don't really understand why this has to flip, but otherwise it yields correlation values above 1...\n",
    "\n",
    "                                ### Create a vector of lags for the correlation function\n",
    "                                lagvec = np.arange(- int(np.round(search_width/dx)), int(search_width/dx) +1,1)# for mode = 'valid'\n",
    "\n",
    "                                ### Convert lag to distance\n",
    "                                shift_vec = lagvec * dx\n",
    "\n",
    "                                ### ID peak correlation coefficient\n",
    "                                ix_peak = np.arange(len(corr_normed))[corr_normed == np.nanmax(corr_normed)][0]\n",
    "                                \n",
    "                                ### Save correlation coefficient, best lag, velocity, etc at the location of peak correlation coefficient\n",
    "                                best_lag = lagvec[ix_peak]\n",
    "                                best_shift = shift_vec[ix_peak]\n",
    "\n",
    "#                                 ### ID peak correlation coefficient; 'raw' = no sub-sampling\n",
    "#                                 plotting = False\n",
    "#                                 best_lag, peak_corr_value = find_correlation_peak(lagvec, shift_vec, corr_normed, max_width, min_width, dx_interp, type, plotting)\n",
    "#                                 best_shift = best_lag * dx\n",
    "\n",
    "                                velocities[rgt][beam][xi] = best_shift/(dt/365)\n",
    "                                correlations[rgt][beam][xi] = corr_normed[ix_peak]\n",
    "                                lags[rgt][beam][xi] = lagvec[ix_peak]\n",
    "                            else:\n",
    "                                ### If there are too many nans, just save a nan\n",
    "                                velocities[rgt][beam][xi] = np.nan\n",
    "                                correlations[rgt][beam][xi] = np.nan\n",
    "                                lags[rgt][beam][xi] = np.nan\n",
    "                                \n",
    "                         \n",
    "                        xy=np.array(pyproj.Proj(3031)(midpoints_lon, midpoints_lat))    \n",
    "                        ### Add velocities to hdf5 file for each beam\n",
    "                        with h5py.File(h5_file_out, 'a') as f:\n",
    "                            f[beam +'/x_atc'] = midpoints_x_atc # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/latitudes'] = midpoints_lat # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/longitudes'] = midpoints_lon # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/velocities'] = velocities[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/correlation_coefficients'] = correlations[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/best_lags'] = lags[rgt][beam] # assign x_atc value of half way along the segment\n",
    "                            f[beam +'/segment_ids'] = midpoints_seg_ids\n",
    "                            f[beam +'/first_cycle_time'] = str(Time(times[cycle1][beam][0]))\n",
    "                            f[beam +'/second_cycle_time'] = str(Time(times[cycle2][beam][0]))\n",
    "                            f[beam +'/Measures_v_along'] = get_measures_along_track_velocity(xy[0], xy[1] , spatial_extent, measures_Vx_path, measures_Vy_path)\n",
    "\n",
    "\n",
    "                    ### Record which cycles contributed to these results\n",
    "                    with h5py.File(h5_file_out, 'a') as f:\n",
    "                        f['contributing_cycles'] = ','.join([cycle1,cycle2])\n",
    "\n",
    "            \n",
    "                total_number_repeat_tracks_processed += 1\n",
    "                \n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f'rgt {rgt} encountered an error')\n",
    "            print(e)\n",
    "            rgts_with_errors.append(rgt)\n",
    "            \n",
    "print(f'Total number of repeat tracks successfully processed = {total_number_repeat_tracks_processed}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old junk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file='/media/rag110/ADATA SD700/ICESat2/output/FIS/rgt0004_veloc0.hdf5'\n",
    "file = '/home/jovyan/shared/surface_velocity/ATL06_out2/rgt0848_veloc0.hdf5'\n",
    "file = '/Users/grace/Dropbox/Cornell/projects/003/ATL06_0848/out/rgt0848_veloc0.hdf5'\n",
    "# file = '/home/jovyan/shared/surface_velocity/ATL06_out2/rgt0491_veloc0.hdf5'\n",
    "\n",
    "# file = '/home/jovyan/shared/surface_velocity/ATL06_out2/rgt0446_veloc0.hdf5'\n",
    "\n",
    "# file = '/home/jovyan/shared/surface_velocity/ATL06_out2/rgt0802_veloc0.hdf5'\n",
    "\n",
    "# file = '/home/jovyan/shared/surface_velocity/ATL06_out2/rgt0537_veloc0.hdf5'\n",
    "\n",
    "# file = '/home/jovyan/shared/surface_velocity/ATL06_out2/rgt0613_veloc0.hdf5'\n",
    "\n",
    "# file = '/home/jovyan/shared/surface_velocity/ATL06_out2/rgt0522_veloc0.hdf5'\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "\n",
    "for ir, rgt in enumerate(rgts.keys()):\n",
    "    if ir < 1e3: #>=0:#20 and ir <=30: # recommend making figures in chunks\n",
    "        correlation_threshold = 0.6\n",
    "\n",
    "        #rgt = file.split('/')[-1][3:7]\n",
    "#         file = '/home/jovyan/shared/surface_velocity/ATL06_out2/rgt' + rgt + '_veloc0.hdf5'\n",
    "        file = '/Users/grace/Dropbox/Cornell/projects/003/ATL06_0848/out/rgt' + rgt + '_veloc0.hdf5'\n",
    "\n",
    "        if glob.glob(file):\n",
    "\n",
    "            ### MOA parameters\n",
    "            moa_datapath = map_data_root #'/srv/tutorial-data/land_ice_applications/'\n",
    "            spatial_extent = np.array([-102, -76, -98, -74.5])\n",
    "            spatial_extent = np.array([-65, -86, -55, -81])\n",
    "\n",
    "            lat=spatial_extent[[1, 3, 3, 1, 1]]\n",
    "            lon=spatial_extent[[2, 2, 0, 0, 2]]\n",
    "            # project the coordinates to Antarctic polar stereographic\n",
    "            xy=np.array(pyproj.Proj(3031)(lon, lat))\n",
    "            # get the bounds of the projected coordinates \n",
    "            XR=[np.nanmin(xy[0,:]), np.nanmax(xy[0,:])]\n",
    "            YR=[np.nanmin(xy[1,:]), np.nanmax(xy[1,:])]\n",
    "            MOA=pc.grid.data().from_geotif(os.path.join(moa_datapath, 'moa_2009_1km.tif'), bounds=[XR, YR])\n",
    "#             MOA=pc.grid.data().from_geotif(os.path.join(moa_datapath, 'MOA','moa_2009_1km.tif'), bounds=[XR, YR])\n",
    "\n",
    "\n",
    "            epsg=3031 #PS?\n",
    "\n",
    "            plt.close('all')\n",
    "            fig = plt.figure(figsize=[11,8])\n",
    "            grid = plt.GridSpec(6, 2, wspace=0.4, hspace=0.3)\n",
    "            haxMOA=fig.add_subplot(grid[0:4,1])\n",
    "            MOA.show(ax=haxMOA,cmap='gray', clim=[14000, 17000])\n",
    "\n",
    "\n",
    "            with h5py.File(file, 'r') as f:\n",
    "                    for ib, beam in enumerate(beams):\n",
    "                        hax0=fig.add_subplot(grid[ib,0])\n",
    "                        #1hax1=fig.add_subplot(212)\n",
    "                        #hax1.set_title('measures ' )\n",
    "                        if ib == 0:\n",
    "                            hax0.set_title('velocs vs measures ' + rgt)\n",
    "\n",
    "                        lats = f[f'/{beam}/latitudes'][()]\n",
    "                        lons = f[f'/{beam}/longitudes'][()]\n",
    "                        coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                        velocs = f[f'/{beam}/velocities'][()]\n",
    "                        v_along=f[f'/{beam}/Measures_v_along'][()]\n",
    "                        xy=np.array(pyproj.proj.Proj(3031)(lons,lats))\n",
    "\n",
    "                        ixs0 = coeffs <= correlation_threshold\n",
    "                        ixs = coeffs > correlation_threshold\n",
    "\n",
    "\n",
    "\n",
    "                        h0 = hax0.scatter(xy[0], velocs, 1, coeffs, vmin = 0, vmax = 1,cmap = 'viridis')\n",
    "                        h1 = hax0.plot(xy[0], v_along, 'k-')\n",
    "\n",
    "                        hax0.set_ylim(-800,800)\n",
    "                        c = plt.colorbar(h0, ax = hax0)\n",
    "                        c.set_label('Correlation coefficient (0 -> 1)')\n",
    "\n",
    "                        h2 = haxMOA.scatter(xy[0][ixs0], xy[1][ixs0], 0.02, 'k')\n",
    "                        h3 = haxMOA.scatter(xy[0][ixs], xy[1][ixs], 0.15, velocs[ixs], vmin = -800, vmax = 800,cmap = 'plasma')\n",
    "\n",
    "\n",
    "\n",
    "            c = plt.colorbar(h3, ax = haxMOA)\n",
    "            c.set_label('Along-track velocity (m/yr)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            outfile = out_path + 'rgt' + rgt + '.' + beam + '_vs_measures.png'\n",
    "            plt.savefig(outfile, dpi = 200)\n",
    "            plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate by ascending and descending tracks; masked by correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot MOA with correlation coefficient and velocity on top, separated by ascending and descending tracks\n",
    "plt.close('all')\n",
    "fig0 = plt.figure(figsize=[8,8])\n",
    "hax0=fig0.add_subplot(211, aspect='equal')\n",
    "MOA.show(ax=hax0,cmap='gray', clim=[14000, 17000])\n",
    "hax1=fig0.add_subplot(212, aspect='equal')\n",
    "MOA.show(ax=hax1,cmap='gray', clim=[14000, 17000])\n",
    "\n",
    "hax0.set_title('Correlation Coefficient, above correlation threshold ' + str(correlation_threshold))\n",
    "hax1.set_title('Best velocity, above correlation threshold ' + str(correlation_threshold))\n",
    "\n",
    "fig1 = plt.figure(figsize=[8,8])\n",
    "hax2=fig1.add_subplot(211, aspect='equal')\n",
    "MOA.show(ax=hax2,cmap='gray', clim=[14000, 17000])\n",
    "hax3=fig1.add_subplot(212, aspect='equal')\n",
    "MOA.show(ax=hax3,cmap='gray', clim=[14000, 17000])\n",
    "\n",
    "hax3.set_title('Correlation Coefficient, above correlation threshold ' + str(correlation_threshold))\n",
    "hax3.set_title('Best velocity, above correlation threshold ' + str(correlation_threshold))\n",
    "\n",
    "### Loop over results, load data, plot\n",
    "results_files = glob.glob(out_path + '/*.hdf5')\n",
    "for file in results_files:\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                lags = f[f'/{beam}/best_lags'][()]\n",
    "                velocs = f[f'/{beam}/velocities'][()]\n",
    "\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "                ixs = coeffs > correlation_threshold\n",
    "\n",
    "                if np.median(lats[10:20] - lats[9:19]) >=0: # if latitude is increasing\n",
    "                    h0 = hax0.scatter(xy[0][ixs], xy[1][ixs], 0.25, coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                    h1 = hax1.scatter(xy[0][ixs], xy[1][ixs], 0.25, velocs[ixs], vmin = -700, vmax = 700,cmap='RdBu')#,cmap='RdBu')\n",
    "                elif np.median(lats[10:20] - lats[9:19]) <0: # if latitude is decreasing\n",
    "                    h2 = hax2.scatter(xy[0][ixs], xy[1][ixs], 0.25, coeffs[ixs], vmin = correlation_threshold, vmax = 1)\n",
    "                    h3 = hax3.scatter(xy[0][ixs], xy[1][ixs], 0.25, velocs[ixs], vmin = -700, vmax = 700,cmap='RdBu')#,cmap='RdBu')\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "fig0.colorbar(h0, ax = hax0)\n",
    "fig0.colorbar(h1, ax = hax1)\n",
    "fig1.colorbar(h2, ax = hax2)\n",
    "fig1.colorbar(h3, ax = hax3)\n",
    "\n",
    "fig0.suptitle('Descending tracks')\n",
    "fig1.suptitle('Ascending tracks')\n",
    "\n",
    "\n",
    "outfile = out_path + 'results_masked_descending.png'\n",
    "fig0.savefig(outfile)\n",
    "outfile = out_path + 'results_masked_ascending.png'\n",
    "fig1.savefig(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spatial_extent = np.array([-65, -86, -55, -81])\n",
    "#Rodrigo Gomez Fell computer path @ UC\n",
    "path = '/mnt/user1/Antarctica/Quantarctica3/Glaciology/MEaSUREs Ice Flow Velocity/'\n",
    "vel_x = 'anta_phase_map_VX.tif'\n",
    "vel_y = 'anta_phase_map_VY.tif'\n",
    "data_points = {}\n",
    "temp = []\n",
    "for file in results_files:\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        for beam in beams:\n",
    "            try:\n",
    "                lats = f[f'/{beam}/latitudes'][()]\n",
    "                lons = f[f'/{beam}/longitudes'][()]\n",
    "                coeffs = f[f'/{beam}/correlation_coefficients'][()]\n",
    "                lags = f[f'/{beam}/best_lags'][()]\n",
    "                velocs = f[f'/{beam}/velocities'][()]\n",
    "\n",
    "                xy=np.array(pyproj.proj.Proj(epsg)(lons,lats))\n",
    "                ixs = coeffs > correlation_threshold\n",
    "                data_points['x']=xy[0,:].reshape(lats.shape)\n",
    "                data_points['y']=xy[1,:].reshape(lons.shape)\n",
    "                \n",
    "                temp.append(add_surface_velocity_to_is2_dict(data_points, spatial_extent, path, vel_x, vel_y ))\n",
    "               \n",
    "            except:\n",
    "                \n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.scatter(velocs, temp['v_along'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Look at the difference between temp Vdiff in temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
